{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700823db",
   "metadata": {},
   "source": [
    "# GenAI-Camp: Day 03\n",
    "## Lesson: Augmentation\n",
    "\n",
    "This lesson is intended to show you how to enrich prompts with internal knowledge\n",
    "\n",
    "During this lesson you will learn ...\n",
    "\n",
    "- how to augment prompts with external knowledge\n",
    "- about the disadvantages of uploading whole, large files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc88403",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "Import the necessary libraries, set constants, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e026c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7fde9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running on LOCAL environment.\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   from google.colab import userdata\n",
    "   GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\")\n",
    "else:\n",
    "   from dotenv import load_dotenv, find_dotenv\n",
    "   load_dotenv(find_dotenv())\n",
    "   GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11c379dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of ressources\n",
    "if COLAB:\n",
    "    # Clone the data repository into colab\n",
    "    !git clone https://github.com/openknowledge/workshop-genai-camp-data.git\n",
    "    DATA_PATH = \"/content/workshop-genai-camp-data/day-03/data\"\n",
    "else:\n",
    "    DATA_PATH = \"../data\"\n",
    "BOOK_FILE = DATA_PATH + \"/study_in_scarlett.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f48c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_MODEL = \"gemini-2.0-flash\"\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9\n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200\n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT = \" \"\n",
    "\n",
    "# Define a function to generate completions using the Gemini model\n",
    "def generate_gemini_completion(\n",
    "        model_name: str = DEFAULT_MODEL, \n",
    "        temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "        top_k: int = DEFAULT_CONFIG_TOP_K, \n",
    "        max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS, \n",
    "        system_prompt : str = DEFAULT_SYSTEM_PROMPT, \n",
    "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "        txt_file: str | None = None,\n",
    "        verbose: bool = False\n",
    "        ) -> str: \n",
    "    \n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_GEMINI_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    txt_file : str, optional [default: None]\n",
    "        The path to a text file to use as input for the completion\n",
    "        If None, the user_prompt is used instead\n",
    "        If provided, the user_prompt is appended to the file content\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False            \n",
    "    Returns \n",
    "    -------\n",
    "    str :\n",
    "        the generated text      \n",
    "    \"\"\"    \n",
    "    \n",
    "    # create generation config \n",
    "    model_config = types.GenerateContentConfig(\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        system_instruction=system_prompt,\n",
    "    )\n",
    "\n",
    "    # Append file content if provided\n",
    "    contents = user_prompt\n",
    "    if txt_file is not None:\n",
    "        contents = [\n",
    "            types.Part.from_bytes(\n",
    "                data=Path(txt_file).read_bytes(),\n",
    "                mime_type='text/plain',\n",
    "            ),\n",
    "            user_prompt\n",
    "        ]\n",
    "    \n",
    "    # create generation request\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=contents,\n",
    "        config=model_config,\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Input tokens count: {response.usage_metadata.prompt_token_count}\")\n",
    "        print(f\"Total tokens count: {response.usage_metadata.total_token_count}\")\n",
    "      \n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f37f74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In \"A Study in Scarlet,\" the number written on the wall (not the ceiling) in blood at the crime scene is **RACHE**.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's test the models knowledge on a book\n",
    "user_input = \"In Sherlock Holmes book 'A Study in Scarlett': Which number was written on the ceiling?\"\n",
    "generate_gemini_completion(user_prompt=user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a85344",
   "metadata": {},
   "source": [
    "### Exercise 01: Provide internal knowledge per file upload\n",
    "In order to provide internal knowldge, gemini (and others) support uploading of documents. Your task is to provide the BOOK_FILE, representing internal knowledge alongside the prompt. Do you find a way of getting insights to token usage?  \n",
    "**Hints**:\n",
    "* See the implementation of the provided function for insights of token usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2be0daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens count: 62191\n",
      "Total tokens count: 62203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The number 28 was written on the ceiling.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question about a detailed part of the book\n",
    "user_prompt = \"Which number was written on the ceiling?\"\n",
    "\n",
    "# TODO: Call the gemini model with the user prompt and the book file\n",
    "generate_gemini_completion(user_prompt=user_input, txt_file=BOOK_FILE, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc9939c",
   "metadata": {},
   "source": [
    "### Exercise 02: Enhance prompt with internal knowledge\n",
    "Providing small pieces of context within the prompt should be the best way to answer this user query. Your task is to simply enhance the prompt below by providing the internal knowledge about the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f69fa540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens count: 141\n",
      "Total tokens count: 153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The number 28 was written on the ceiling.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A paragraph from the book. which should be enough to answer the question\n",
    "internal_book_knowledge_chunk = \"\"\"\n",
    "Still more shaken was he next morning. They had sat down to their\n",
    "breakfast when Lucy with a cry of surprise pointed upwards. In the\n",
    "centre of the ceiling was scrawled, with a burned stick apparently, the\n",
    "number 28. To his daughter it was unintelligible, and he did not\n",
    "enlighten her. That night he sat up with his gun and kept watch and\n",
    "ward. He saw and he heard nothing, and yet in the morning a great 27\n",
    "had been painted upon the outside of his door.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"Which number was written on the ceiling?\"\n",
    "\n",
    "# TODO: Create a augmented prompt, where the user prompt is augmented with the internal book knowledge chunk\n",
    "augmented_user_prompt = f\"Answer the question below based on the context.\\n\\nContext: {internal_book_knowledge_chunk}\\n\\nQuestion: {user_prompt}\"\n",
    "generate_gemini_completion(user_prompt=augmented_user_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6557010",
   "metadata": {},
   "source": [
    "### Exercise 03: Calculate costs for both calls\n",
    "Costs is one factor, which should prevent us from uploading large file. But how much is it exactly? And how much more is it than just uploading small amounts of internal knowledge? Use only input token for calculation. Let's say we use the paid tier.  \n",
    "**Hints**:  \n",
    "* See [pricing](https://ai.google.dev/gemini-api/docs/pricing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c10bb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Costs for 62191 tokens using file upload: 0.00622 €\n",
      "Costs for 141 tokens using chunk upload: 1e-05 €\n",
      "Uploading the file is 441 times more expensive than uploading the chunk.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Calculate costs for input tokens for both calls and compare them\n",
    "def calculate_costs(token_count: int, price_per_million_tokens: float = 1.25) -> float:\n",
    "    \"\"\" Calculate the costs of a given number of tokens based on the price per million tokens \"\"\"\n",
    "    return token_count * (price_per_million_tokens / 1000000)\n",
    "\n",
    "# Token counts for the two calls\n",
    "input_token_count_file_upload = 62191\n",
    "input_token_count_chunk_upload = 141\n",
    "\n",
    "# For gemini-2.0-flash the price is 0.1 € per million tokens\n",
    "euro_per_million_tokens = 0.1\n",
    "\n",
    "costs_file_upload = calculate_costs(token_count=input_token_count_file_upload, price_per_million_tokens=euro_per_million_tokens)\n",
    "costs_chunk_upload = calculate_costs(token_count=input_token_count_chunk_upload, price_per_million_tokens=euro_per_million_tokens)\n",
    "\n",
    "print(f\"Costs for {input_token_count_file_upload} tokens using file upload: {round(costs_file_upload, 5)} €\")\n",
    "print(f\"Costs for {input_token_count_chunk_upload} tokens using chunk upload: {round(costs_chunk_upload, 5)} €\")\n",
    "print(f\"Uploading the file is {round(costs_file_upload/costs_chunk_upload)} times more expensive than uploading the chunk.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
