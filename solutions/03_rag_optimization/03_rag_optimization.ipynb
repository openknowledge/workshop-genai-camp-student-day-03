{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ba3deb",
   "metadata": {},
   "source": [
    "# GenAI-Camp: Day 03\n",
    "## Lesson: RAG Optimization\n",
    "\n",
    "This lesson is intended to show you how to optimize a RAG system.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- use HYDE for generating hypothetical documents to potentially boost the retrieval\n",
    "- use a reranker to improve RAG performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac7e55",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "Import the necessary libraries, set constants, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8945438f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running on LOCAL environment.\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   from google.colab import userdata\n",
    "   GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\")\n",
    "else:\n",
    "   from dotenv import load_dotenv, find_dotenv\n",
    "   load_dotenv(find_dotenv())\n",
    "   GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "   JINA_API_KEY = os.getenv(\"JINA_API_KEY\")\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7afd0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional libraries\n",
    "if COLAB:\n",
    "  !pip install -qU chromadb\n",
    "    \n",
    "# Import additional libraries\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9302ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of ressources\n",
    "if COLAB:\n",
    "    # Clone the data repository into colab\n",
    "    !git clone https://github.com/openknowledge/workshop-genai-camp-data.git\n",
    "    %cd workshop-genai-camp-data\n",
    "    !git lfs pull \n",
    "    ROOT_PATH = \"/content/workshop-genai-camp-data/day-03\"\n",
    "else:\n",
    "    ROOT_PATH = \"..\"\n",
    "DATA_PATH = ROOT_PATH + \"/data\"\n",
    "EVALUATION_PATH = ROOT_PATH + \"/evaluation\"\n",
    "KNOWLEDGEBASE_PATH = ROOT_PATH + \"/knowledgebase\"\n",
    "BOOK_CATALOG_FILE = DATA_PATH + \"/books.json\"\n",
    "TESTDATA_FILE = EVALUATION_PATH + \"/synthetic_testset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9414c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default models\n",
    "GENERATION_MODEL = \"gemini-1.5-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
    "\n",
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9 \n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200 \n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT = \" \"\n",
    "\n",
    "# Set defaults for rag\n",
    "DEFAULT_K = 3\n",
    "DEFAULT_CHUNK_SIZE = 2000\n",
    "DEFAULT_CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "977b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_completion(\n",
    "        model_name: str = GENERATION_MODEL, \n",
    "        temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "        top_k: int = DEFAULT_CONFIG_TOP_K, \n",
    "        max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS, \n",
    "        system_prompt : str = DEFAULT_SYSTEM_PROMPT, \n",
    "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "        verbose: bool = False\n",
    "        ) -> str: \n",
    "    \n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_GEMINI_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False            \n",
    "    Returns \n",
    "    -------\n",
    "    str :\n",
    "        the generated text      \n",
    "    \"\"\"    \n",
    "    if verbose: \n",
    "        # print out summary of input values / parameters\n",
    "        print(f'Generating answer for following config:')\n",
    "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
    "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
    "        print(f'  - MODEL used:\\n {model_name} (temperature = {temperature}, top_k = {top_k}, max_output_tokens = {max_output_tokens})')\n",
    "\n",
    "    # create generation config \n",
    "    model_config = types.GenerateContentConfig(\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        system_instruction=system_prompt,\n",
    "    )\n",
    "    \n",
    "    # create generation request\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=user_prompt,\n",
    "        config=model_config,\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c14c9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_objects_from_json(file_path: str, cls: BaseModel) -> list:\n",
    "    \"\"\"Reads list of objects from a JSON file and returns the list.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        objects = [cls(**item) for item in data]\n",
    "    return objects\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"Represents the metadata of a document which is stored in the knowledgebase.\"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    pub_year: int\n",
    "\n",
    "class Book(BaseModel):\n",
    "    \"\"\"Represents a book with its metadata.\"\"\"\n",
    "    metadata: Metadata\n",
    "    summary: str\n",
    "\n",
    "class TestdataItem(BaseModel):\n",
    "    \"\"\"Represents a test data item with its input, output, and source.\"\"\"\n",
    "    input: str\n",
    "    output: str\n",
    "    source: Book   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "916cfda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG building blocks\n",
    "\n",
    "# This will be the chromadb collection we use as a knowledge base. We do not need the client.\n",
    "chromadb_collection = PersistentClient(path=KNOWLEDGEBASE_PATH).get_or_create_collection(name=\"default\")\n",
    "\n",
    "class FetchedChunk(BaseModel):\n",
    "    \"\"\"Represents a chunk fetched from the knowledgebase.\"\"\"\n",
    "    chunk: str\n",
    "    metadata: Metadata\n",
    "\n",
    "# Building Block \"Embedding\": Create multi dimensional embeddings for a given chunk.\n",
    "def do_embed(chunk: str) -> list[float]:\n",
    "  \"\"\" Embeds a given chunk and returns the embedding\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunk : str\n",
    "      The chunk to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embedding: [float]\n",
    "      The created embedding\n",
    "  \"\"\"\n",
    "  content_embeddings = client.models.embed_content(model=EMBEDDING_MODEL, contents=chunk).embeddings\n",
    "  return content_embeddings[0].values\n",
    "\n",
    "# Building Block \"Augmentation\": Create an updated prompt by merging the original user input with the provided context\n",
    "# Attention: We manipulated the augmented prompt in order to see the guardrails in action\n",
    "def augment(user_input: str, context: list[str]) -> str:\n",
    "  \"\"\" Augments a given user input by merging it with the provided context and returns the augmented prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be augmented\n",
    "  context : [str]\n",
    "      The context to be merged with the user input\n",
    "  Returns\n",
    "  -------\n",
    "  augmented_prompt: str\n",
    "      The created augmented prompt\n",
    "  \"\"\"\n",
    "  prepared_context = \"\\n\".join(context)\n",
    "  augmented_prompt = f\"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n{prepared_context}?\\n\n",
    "    Question: \\n{user_input}\\n\n",
    "\n",
    "    Answer:\n",
    "  \"\"\"\n",
    "  return augmented_prompt\n",
    "\n",
    "# Building Block \"Top-k Fetching\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "def do_top_k_fetching(user_input_embedding: list[float], top_k: int) -> list[FetchedChunk]:\n",
    "  \"\"\" Fetches the k semantically closest chunks to the user input from the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input_embedding : [float]\n",
    "      The embedding of the user input\n",
    "  top_k : int\n",
    "      The number of semantically closest chunks to be fetched\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Since we will do the fetching always only for one user_input,\n",
    "  # instead of querying for multiple embeddings simultanously as allowed by the choma API,\n",
    "  # we add the embeddings below to a list and return only the first document (chunk)\n",
    "  \n",
    "  query_result = chromadb_collection.query(\n",
    "      query_embeddings=[user_input_embedding],\n",
    "      n_results=top_k,\n",
    "  )\n",
    "  chunks = query_result[\"documents\"][0]\n",
    "  metadatas = query_result[\"metadatas\"][0]\n",
    "  \n",
    "  fetched_chunks = []\n",
    "  for i in range(len(chunks)):\n",
    "    chunk = chunks[i]\n",
    "    metadata = metadatas[i]\n",
    "    fetched_chunk = FetchedChunk(chunk=chunk, metadata=Metadata(**metadata))\n",
    "    fetched_chunks.append(fetched_chunk)\n",
    "  return fetched_chunks\n",
    "\n",
    "# Building Block \"Generation\": Use the generation model to create a response\n",
    "def generate_response(prompt: str) -> str:\n",
    "  \"\"\" Generates a response for a given prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  prompt : str\n",
    "      The prompt to be used for the generation\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  \"\"\"\n",
    "  return generate_gemini_completion(\n",
    "      model_name=GENERATION_MODEL,\n",
    "      user_prompt=prompt,\n",
    "  )\n",
    "\n",
    "# The rag function should now return the response and the context in order to be evaluated further\n",
    "def do_rag(user_input: str, top_k: int, retrieval_only: bool = False) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(chunk=user_input)\n",
    "\n",
    "  # \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  fetched_chunks = do_top_k_fetching(user_input_embedding=user_input_embedding, top_k=top_k)\n",
    "  context = [chunk.chunk for chunk in fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  if not retrieval_only:\n",
    "    # Generate a response using the augmented prompt\n",
    "    response = generate_response(prompt=augmented_prompt)\n",
    "  else:\n",
    "    response = \"No response generated. Only retrieval was requested.\"\n",
    "\n",
    "  return (response, fetched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7befbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(ground_truth, retrieved: list[str]):\n",
    "    gt_set = set(ground_truth)\n",
    "    retrieved_set = set(retrieved)\n",
    "    return int(bool(gt_set & retrieved_set))\n",
    "\n",
    "def calculate_reciprocal_rank(ground_truth: list[str], retrieved: list[str]) -> float:\n",
    "    gt_set = set(ground_truth)\n",
    "    for rank, item in enumerate(retrieved, start=1):\n",
    "        if item in gt_set:\n",
    "            return 1 / rank\n",
    "    return 0.0\n",
    "    \n",
    "def calculate_precision(ground_truth: list[str], retrieved: list[str]) -> float:\n",
    "    k = len(retrieved)\n",
    "    gt_set = set(ground_truth)\n",
    "    retrieved_set = set(retrieved[:k])\n",
    "    return len(gt_set & retrieved_set) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99f7a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"Represents the evaluation result with its metrics.\"\"\"\n",
    "    mean_hit_rate: float\n",
    "    mean_reciprocal_rank: float\n",
    "    mean_precision: float\n",
    "\n",
    "def run_evaluation(top_k: int, testdata_items: list[TestdataItem], rag_function: any) -> EvaluationResult:\n",
    "    hit_rates = []\n",
    "    reciprocal_ranks = []\n",
    "    precisions = []\n",
    "\n",
    "    for testdata_item in testdata_items:\n",
    "\n",
    "        # Prepare the ground truth\n",
    "        ground_truth = [testdata_item.source.metadata.title]\n",
    "\n",
    "        # Run the RAG pipeline\n",
    "        _, fetched_chunks = rag_function(user_input=testdata_item.input, top_k=top_k, retrieval_only=True)\n",
    "\n",
    "        # Check if the ground truth is in the context\n",
    "        retrieved = [item.metadata.title for item in fetched_chunks]\n",
    "\n",
    "        # Calculate metrics\n",
    "        # Hit rate\n",
    "        hit_rate_value = calculate_hit_rate(ground_truth=ground_truth, retrieved=retrieved)\n",
    "        hit_rates.append(hit_rate_value)\n",
    "        \n",
    "        # Reciprocal rank\n",
    "        reciprocal_rank = calculate_reciprocal_rank(ground_truth=ground_truth, retrieved=retrieved)\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "\n",
    "        # Precision\n",
    "        precision = calculate_precision(ground_truth=ground_truth, retrieved=retrieved)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    # Calculate mean values\n",
    "    mean_hit_rate = sum(hit_rates) / len(hit_rates)\n",
    "    mean_reciprocal_rank = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    mean_precision = sum(precisions) / len(precisions)\n",
    "    return EvaluationResult(\n",
    "        mean_hit_rate=mean_hit_rate,\n",
    "        mean_reciprocal_rank=mean_reciprocal_rank,\n",
    "        mean_precision=mean_precision\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "537b3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data for later evaluating optimization techniques\n",
    "testdata_items = read_objects_from_json(file_path=TESTDATA_FILE, cls=TestdataItem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aaee06",
   "metadata": {},
   "source": [
    "### Exercise 01: Hypothetical Document Embeddings (HyDE)\n",
    "HyDE is a technique for improving retrieval. To do this, hypothetical documents are generated based on the user's input that could potentially solve the user's task. These generated documents should closely resemble the documents or chunks in the knowledge base. The hypothetical, generated documents are then used for semantic retrieval.  \n",
    "The underlying idea is that, although the generated documents may not be factually accurate, they are often semantically closer to relevant knowledge base entries than the user's original query. In practice, multiple hypothetical documents are often generated, embedded, and then averaged over each embedding dimension to create a single embedding used for retrieval. This averaging is intended to neutralize incorrect facts while enhancing semantic similarity to the knowledge base.  \n",
    "Your task is to implement this method. Afterwards, reflect on the potential drawbacks of this approach and evaluate the performance of the resulting system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYDE_GENERATION_MODEL = \"gemini-2.0-flash-lite\"\n",
    "\n",
    "# TODO: Implement a function which creates hypothetical summaries of books from a given user query\n",
    "def generate_hypothetical_documents(user_input: str) -> str:\n",
    "    \"\"\" Generates a hypothetical book summary for a given user input\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user_input : str\n",
    "        The user input to be used for the generation\n",
    "    Returns\n",
    "    -------\n",
    "    response: str\n",
    "        The generated hypothetical book summary\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Use your promp engineering skills to create a prompt for\n",
    "    # generating a hypothetical book summary from a given customer query.\n",
    "    # As an example, you can use a real book summary from the book catalog\n",
    "    # CUSTOMER: \"I’m looking for an old book about a scientist who creates a creature out of dead body parts! The creature comes to life and things go very badly.\"\n",
    "    # SUMMARY:\"Frankenstein; Or, The Modern Prometheus\" by Mary Wollstonecraft Shelley is a novel written in the early 19th century. The story explores themes of ambition, the quest for knowledge, and the consequences of man's hubris through the experiences of Victor Frankenstein and the monstrous creation of his own making.The opening of the book introduces Robert Walton, an ambitious explorer on a quest to discover new lands and knowledge in the icy regions of the Arctic. In his letters to his sister Margaret, he expresses both enthusiasm and the fear of isolation in his grand venture. As Walton's expedition progresses, he encounters a mysterious, emaciated stranger who has faced great suffering—furthering the intrigue of his narrative. This stranger ultimately reveals his tale of creation, loss, and the profound consequences of seeking knowledge that lies beyond human bounds. The narrative is set up in a manner that suggests a deep examination of the emotions and ethical dilemmas faced by those who dare to defy the natural order.\n",
    "    hyde_prompt = f\"\"\"\n",
    "        You are HYDE, a fictional book summary creator for a bookstore assistant.\n",
    "        When a customer describes a book they are trying to remember or find, \n",
    "        you will generate a summary of the book based on the description provided.\n",
    "        Use your extensive knowledge of literature to create a plausible summary.\n",
    "\n",
    "        Below is an example:\n",
    "\n",
    "        CUSTOMER:\n",
    "        \"I’m looking for an old book about a scientist who creates a creature out of dead body parts! The creature comes to life and things go very badly.\"\n",
    "\n",
    "        SUMMARY:\n",
    "        \"Frankenstein; Or, The Modern Prometheus\" by Mary Wollstonecraft Shelley is a novel written in the early 19th century. The story explores themes of ambition, the quest for knowledge, and the consequences of man's hubris through the experiences of Victor Frankenstein and the monstrous creation of his own making.\n",
    "        The opening of the book introduces Robert Walton, an ambitious explorer on a quest to discover new lands and knowledge in the icy regions of the Arctic. In his letters to his sister Margaret, he expresses both enthusiasm and the fear of isolation in his grand venture. As Walton's expedition progresses, he encounters a mysterious, emaciated stranger who has faced great suffering—furthering the intrigue of his narrative. This stranger ultimately reveals his tale of creation, loss, and the profound consequences of seeking knowledge that lies beyond human bounds. The narrative is set up in a manner that suggests a deep examination of the emotions and ethical dilemmas faced by those who dare to defy the natural order.\n",
    "\n",
    "        Now, based on the following customer inquiry, write a fictional book summary in the same style and length:\n",
    "\n",
    "        CUSTOMER:\n",
    "        {user_input}\n",
    "        \n",
    "        SUMMARY:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a hypothetical book summary using the user input as a prompt\n",
    "    hypothetical_summary = generate_gemini_completion(\n",
    "        model_name=HYDE_GENERATION_MODEL,\n",
    "        user_prompt=hyde_prompt,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    \n",
    "    return hypothetical_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the following function to use the new generate_hypothetical_documents function\n",
    "def do_hyde_rag(user_input: str, top_k: int, retrieval_only: bool = False, verbose: bool = False) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "\n",
    "  # TODO: Generate a hypothetical book summary using the user input as a prompt\n",
    "  hypothetical_summary = generate_hypothetical_documents(user_input=user_input)\n",
    "\n",
    "  # TODO: Embed the hypothetical book summaries\n",
    "  hypothetical_summary_embedding = do_embed(chunk=hypothetical_summary)\n",
    "\n",
    "  # TODO: \"R\" like \"Retrieval\": Get the top-k semantically closest chunks to the hypothetical summary\n",
    "  fetched_chunks = do_top_k_fetching(user_input_embedding=hypothetical_summary_embedding, top_k=top_k)\n",
    "  context = [chunk.chunk for chunk in fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  if not retrieval_only:\n",
    "    # Generate a response using the augmented prompt\n",
    "    response = generate_response(prompt=augmented_prompt)\n",
    "  else:\n",
    "    response = \"No response generated. Only retrieval was requested.\"\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"User input: {user_input}\")\n",
    "    print(f\"Generated hypothetical summary: {hypothetical_summary}\")\n",
    "    print(f\"Fetched chunks: {fetched_chunks}\")\n",
    "    print(f\"Augmented prompt: {augmented_prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "  return (response, fetched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd1b330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: I'm looking for a book that takes place in London. I think it's about a crime that happened on a foggy night?\n",
      "Generated hypothetical summary: \"The Hound of the Baskervilles\" by Sir Arthur Conan Doyle is a thrilling mystery novel, set in the atmospheric backdrop of the English moors and, specifically, London. The story follows the astute detective Sherlock Holmes and his loyal companion, Dr. Watson, as they investigate a mysterious curse and a series of eerie deaths plaguing the Baskerville family.\n",
      "\n",
      "The narrative begins with the ominous tale of a spectral hound, said to haunt the Baskerville estate, and the suspicious death of Sir Charles Baskerville. Fearing for the life of the new heir, Sir Henry Baskerville, Holmes and Watson embark on a journey to unravel the truth behind the legend. The investigation takes them to the desolate moors of Devonshire, where they encounter a cast of eccentric characters, including the enigmatic Stapleton and the elusive escaped convict. The fog-laden nights and the desolate landscape contribute to a palpable sense of suspense and foreboding as Holmes and Watson piece together clues. Ultimately, they uncover a complex\n",
      "Fetched chunks: [FetchedChunk(chunk='\"The Hound of the Baskervilles\" by Arthur Conan Doyle is a detective novel written during the late 19th century. This classic work features the iconic detective Sherlock Holmes and his loyal friend Dr. John Watson as they embark on a chilling investigation involving a legendary supernatural creature that haunts the Baskerville family. The story is set against the eerie backdrop of the English moors, where mystery and danger intertwine.  The opening of the novel introduces us to Sherlock Holmes and Dr. Watson, who are in the midst of addressing an intriguing case brought forth by Dr. James Mortimer, a country practitioner. Mortimer presents a cane belonging to a mysterious visitor, prompting Holmes to deduce details about the man based solely on the cane’s features. Their conversation soon shifts to the ominous legend of the Baskervilles and the recent suspicious death of Sir Charles Baskerville. As secrets unravel concerning the eerie curse that has plagued the Baskerville family and odd happenings that surround Sir Charles\\'s heir, Sir Henry Baskerville, the stage is set for a gripping tale of suspense and intrigue. Holmes and Watson are drawn into a web of supernatural terror and human malevolence as they seek to protect the heir from the dark legacy of the Baskerville line.', metadata=Metadata(url='https://www.gutenberg.org/ebooks/2852', title='The Hound of the Baskervilles', pub_year=2001)), FetchedChunk(chunk='\"The Hound of the Baskervilles\" by Sir Arthur Conan Doyle is a detective novel written in the late 19th century, characterized by the intriguing collaboration between the iconic detective Sherlock Holmes and his trusty companion, Dr. John Watson. The story revolves around the mysterious death of Sir Charles Baskerville, which is shrouded in a legend involving a supernatural hound that allegedly haunts the Baskerville family. As the plot unfolds, Watson and Holmes delve into the circumstances surrounding Sir Charles\\'s demise and the potential threats faced by his heir, Sir Henry Baskerville.  At the start of the novel, we are introduced to Sherlock Holmes as he analyzes a visitor\\'s cane left behind during his consultation. The visitor, Dr. James Mortimer, reveals troubling information about Sir Charles Baskerville\\'s recent death and presents an old family manuscript detailing the curse of the Baskerville hound. This manuscript raises escalated concerns about Sir Henry Baskerville, who is set to inherit the estate and return to Baskerville Hall. The tension builds as it becomes clear that there may be someone, or something, attempting to thwart Henry\\'s arrival, leading to a blend of human and supernatural intrigue that sets the stage for Holmes and Watson\\'s investigation into the dark past of the Baskerville lineage.', metadata=Metadata(url='https://www.gutenberg.org/ebooks/3070', title='The Hound of the Baskervilles', pub_year=2002)), FetchedChunk(chunk='\"The Valley of Fear\" by Sir Arthur Conan Doyle is a detective novel written during the late 19th century. This work features the famous detective Sherlock Holmes and his companion Dr. John Watson as they embark on a complex mystery involving murder, secret codes, and a shadowy figure from the past. The narrative intertwines elements of crime investigation with themes of deception and treachery, as the characters navigate a web of suspicion surrounding the victim, John Douglas.   At the start of the story, Watson observes Holmes in a state of intense thought after receiving a cipher from a mysterious informant named Porlock, suggesting impending danger for Douglas. Holmes uncovers that Douglas has been fatally shot in his home, Birlstone Manor. In the aftermath, key characters such as the local police inspector, various household staff, and the enigmatic Cecil Barker come into play, each offering insights and evidence that point towards a deeper conspiracy tied to Douglas\\'s past in America. As Holmes pieces together the clues, he reveals a connection to a secret society and a possible betrayal, setting the stage for a thrilling investigation that blends intellectual challenge with human intrigue.', metadata=Metadata(url='https://www.gutenberg.org/ebooks/3776', title='The Valley of Fear', pub_year=2003))]\n",
      "Augmented prompt: \n",
      "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
      "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\n",
      "\n",
      "\n",
      "    Context:\n",
      "\"The Hound of the Baskervilles\" by Arthur Conan Doyle is a detective novel written during the late 19th century. This classic work features the iconic detective Sherlock Holmes and his loyal friend Dr. John Watson as they embark on a chilling investigation involving a legendary supernatural creature that haunts the Baskerville family. The story is set against the eerie backdrop of the English moors, where mystery and danger intertwine.  The opening of the novel introduces us to Sherlock Holmes and Dr. Watson, who are in the midst of addressing an intriguing case brought forth by Dr. James Mortimer, a country practitioner. Mortimer presents a cane belonging to a mysterious visitor, prompting Holmes to deduce details about the man based solely on the cane’s features. Their conversation soon shifts to the ominous legend of the Baskervilles and the recent suspicious death of Sir Charles Baskerville. As secrets unravel concerning the eerie curse that has plagued the Baskerville family and odd happenings that surround Sir Charles's heir, Sir Henry Baskerville, the stage is set for a gripping tale of suspense and intrigue. Holmes and Watson are drawn into a web of supernatural terror and human malevolence as they seek to protect the heir from the dark legacy of the Baskerville line.\n",
      "\"The Hound of the Baskervilles\" by Sir Arthur Conan Doyle is a detective novel written in the late 19th century, characterized by the intriguing collaboration between the iconic detective Sherlock Holmes and his trusty companion, Dr. John Watson. The story revolves around the mysterious death of Sir Charles Baskerville, which is shrouded in a legend involving a supernatural hound that allegedly haunts the Baskerville family. As the plot unfolds, Watson and Holmes delve into the circumstances surrounding Sir Charles's demise and the potential threats faced by his heir, Sir Henry Baskerville.  At the start of the novel, we are introduced to Sherlock Holmes as he analyzes a visitor's cane left behind during his consultation. The visitor, Dr. James Mortimer, reveals troubling information about Sir Charles Baskerville's recent death and presents an old family manuscript detailing the curse of the Baskerville hound. This manuscript raises escalated concerns about Sir Henry Baskerville, who is set to inherit the estate and return to Baskerville Hall. The tension builds as it becomes clear that there may be someone, or something, attempting to thwart Henry's arrival, leading to a blend of human and supernatural intrigue that sets the stage for Holmes and Watson's investigation into the dark past of the Baskerville lineage.\n",
      "\"The Valley of Fear\" by Sir Arthur Conan Doyle is a detective novel written during the late 19th century. This work features the famous detective Sherlock Holmes and his companion Dr. John Watson as they embark on a complex mystery involving murder, secret codes, and a shadowy figure from the past. The narrative intertwines elements of crime investigation with themes of deception and treachery, as the characters navigate a web of suspicion surrounding the victim, John Douglas.   At the start of the story, Watson observes Holmes in a state of intense thought after receiving a cipher from a mysterious informant named Porlock, suggesting impending danger for Douglas. Holmes uncovers that Douglas has been fatally shot in his home, Birlstone Manor. In the aftermath, key characters such as the local police inspector, various household staff, and the enigmatic Cecil Barker come into play, each offering insights and evidence that point towards a deeper conspiracy tied to Douglas's past in America. As Holmes pieces together the clues, he reveals a connection to a secret society and a possible betrayal, setting the stage for a thrilling investigation that blends intellectual challenge with human intrigue.?\n",
      "\n",
      "    Question: \n",
      "I'm looking for a book that takes place in London. I think it's about a crime that happened on a foggy night?\n",
      "\n",
      "\n",
      "    Answer:\n",
      "  \n",
      "Generated response: No response generated. Only retrieval was requested.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test the do_hyde_rag function with a user input\n",
    "user_prompt = \"I'm looking for a book that takes place in London. I think it's about a crime that happened on a foggy night?\" \n",
    "response, context = do_hyde_rag(user_input=user_prompt, top_k=DEFAULT_K, retrieval_only=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4da135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationResult(mean_hit_rate=0.43333333333333335, mean_reciprocal_rank=0.3555555555555555, mean_precision=0.14444444444444443)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Evaluate the new system. How does it perform compared to the original RAG system? What might be the reasons for the differences?\n",
    "evaluation_result = run_evaluation(\n",
    "    top_k=DEFAULT_K,\n",
    "    testdata_items=testdata_items,\n",
    "    rag_function=do_hyde_rag\n",
    ")\n",
    "\n",
    "print(f\"Mean Hit Rate: {evaluation_result.mean_hit_rate:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank: {evaluation_result.mean_reciprocal_rank:.2f}\")\n",
    "print(f\"Mean Precision: {evaluation_result.mean_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e7684",
   "metadata": {},
   "source": [
    "### Exercise 02: Reranking\n",
    "In a RAG system, rerankers play a crucial role in improving the quality of retrieved information before it's passed to the language model. While the retriever fetches potentially relevant documents based on semantic similarity, it often returns noisy or loosely related results. Rerankers evaluate and reorder these documents using more advanced scoring—often based on cross-encoders or task-specific relevance signals—ensuring that the most relevant, contextually appropriate content is prioritized. This leads to more accurate, coherent, and trustworthy generated outputs.  \n",
    "Your task is to use a reranker from Hugging Face to reorder the retrieved chunks. Then, integrate the reranking step into the existing RAG function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77b66289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this the first time, get your free API-Key at https://jina.ai/reranker/\n",
    "if COLAB:\n",
    "    JINA_API_KEY=userdata.get('JINA_API_KEY')\n",
    "\n",
    "# Here we use the Jina API to rerank the documents\n",
    "def use_reranker(query:str, documents:list[str], top_n: int) -> list[int]:\n",
    "  url = \"https://api.jina.ai/v1/rerank\"\n",
    "  headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {JINA_API_KEY}\"\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "      \"model\": \"jina-reranker-v2-base-multilingual\",\n",
    "      \"query\": query,\n",
    "      \"top_n\": top_n,\n",
    "      \"documents\": documents,\n",
    "      \"return_documents\": False\n",
    "  }\n",
    "\n",
    "  # Make the request\n",
    "  response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "  results = json.loads(response.text)[\"results\"]\n",
    "  return [result[\"index\"] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a293a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the reranking function, which uses the CrossEncoder model to rerank the fetched chunks\n",
    "def rerank(user_input: str, fetched_chunks: list[FetchedChunk]) -> list[FetchedChunk]:\n",
    "    \"\"\" Reranks the fetched chunks based on the user input and returns the reranked chunks\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_input : str\n",
    "        The user input to be used for the reranking\n",
    "    fetched_chunks : [FetchedChunk]\n",
    "        The fetched chunks to be reranked\n",
    "    Returns\n",
    "    -------\n",
    "    reranked_chunks: [FetchedChunk]\n",
    "        The reranked chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rank the fetched chunks based on the user input\n",
    "    documents = [chunk.chunk for chunk in fetched_chunks]\n",
    "    rankings = use_reranker(\n",
    "        query=user_input,\n",
    "        documents=documents,\n",
    "        top_n=len(documents)\n",
    "    )\n",
    "\n",
    "    # TODO: Sort the fetched chunks based on the rankings and return the reranked chunks\n",
    "    return [fetched_chunks[i] for i in rankings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fdbb4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the do_reranked_rag function to use the reranking function\n",
    "def do_reranked_rag(user_input: str, top_k: int, retrieval_only: bool = False, verbose: bool = False) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(chunk=user_input)\n",
    "\n",
    "  # \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  fetched_chunks = do_top_k_fetching(user_input_embedding=user_input_embedding, top_k=top_k)\n",
    "\n",
    "  # TODO: Rerank the fetched chunks based on the user input\n",
    "  ranked_fetched_chunks = rerank(user_input=user_input, fetched_chunks=fetched_chunks)\n",
    "  context = [chunk.chunk for chunk in ranked_fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  if not retrieval_only:\n",
    "    # Generate a response using the augmented prompt\n",
    "    response = generate_response(prompt=augmented_prompt)\n",
    "  else:\n",
    "    response = \"No response generated. Only retrieval was requested.\"\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"User input: {user_input}\")\n",
    "    print(f\"Fetched chunks: {fetched_chunks}\")\n",
    "    print(f\"Ranked fetched chunks: {ranked_fetched_chunks}\")\n",
    "    print(f\"Augmented prompt: {augmented_prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "  return (response, ranked_fetched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "635cf617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Hit Rate: 0.83\n",
      "Mean Reciprocal Rank: 0.78\n",
      "Mean Precision: 0.28\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluate the new system. How does it perform compared to the original RAG system?\n",
    "evaluation_result = run_evaluation(\n",
    "    top_k=DEFAULT_K,\n",
    "    testdata_items=testdata_items,\n",
    "    rag_function=do_reranked_rag\n",
    ")\n",
    "print(f\"Mean Hit Rate: {evaluation_result.mean_hit_rate:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank: {evaluation_result.mean_reciprocal_rank:.2f}\")\n",
    "print(f\"Mean Precision: {evaluation_result.mean_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac72aef",
   "metadata": {},
   "source": [
    "### Exercise 03: Improving Reranked RAG\n",
    "Using the reranker seems to improve quality. Unfortunately, precision is still very low. Can you think about a way to keep reciprocal rank and hit rate high, while also increasing precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf0aec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve the rag system even further. How can we improve precision while keeping hit rate and reciprocal rank high?\n",
    "def do_reranked_rag_v2(user_input: str, top_k: int, retrieval_only: bool = False, verbose: bool = False) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(chunk=user_input)\n",
    "\n",
    "  # \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  # TODO: Fetch more chunks than before\n",
    "  fetched_chunks = do_top_k_fetching(user_input_embedding=user_input_embedding, top_k=15) \n",
    "\n",
    "  # TODO: Rerank the fetched chunks and only keep the top_k chunks\n",
    "  ranked_fetched_chunks = rerank(user_input=user_input, fetched_chunks=fetched_chunks)[:top_k]\n",
    "  context = [chunk.chunk for chunk in ranked_fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  if not retrieval_only:\n",
    "    # Generate a response using the augmented prompt\n",
    "    response = generate_response(prompt=augmented_prompt)\n",
    "  else:\n",
    "    response = \"No response generated. Only retrieval was requested.\"\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"User input: {user_input}\")\n",
    "    print(f\"Fetched chunks: {fetched_chunks}\")\n",
    "    print(f\"Ranked fetched chunks: {ranked_fetched_chunks}\")\n",
    "    print(f\"Augmented prompt: {augmented_prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "  return (response, ranked_fetched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00e80464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Hit Rate: 0.90\n",
      "Mean Reciprocal Rank: 0.82\n",
      "Mean Precision: 0.30\n"
     ]
    }
   ],
   "source": [
    "# TODO: Evaluate the new system. How does it perform compared to the original RAG system?\n",
    "evaluation_result = run_evaluation(\n",
    "    top_k=DEFAULT_K,\n",
    "    testdata_items=testdata_items,\n",
    "    rag_function=do_reranked_rag_v2\n",
    ")\n",
    "print(f\"Mean Hit Rate: {evaluation_result.mean_hit_rate:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank: {evaluation_result.mean_reciprocal_rank:.2f}\")\n",
    "print(f\"Mean Precision: {evaluation_result.mean_precision:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
