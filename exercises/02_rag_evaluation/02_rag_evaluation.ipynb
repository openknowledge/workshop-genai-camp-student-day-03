{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c490e27",
   "metadata": {},
   "source": [
    "# GenAI-Camp: Day 03\n",
    "## Lesson: RAG Evaluation\n",
    "\n",
    "This lesson is intended to show you how to evaluate a RAG system.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- synthetically generate test data\n",
    "- use metrics for evaluating the retrieval\n",
    "- visually compare different configurations of the RAG system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac7e55",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "Import the necessary libraries, set constants, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from time import sleep\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   from google.colab import userdata\n",
    "   GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\")\n",
    "else:\n",
    "   from dotenv import load_dotenv, find_dotenv\n",
    "   load_dotenv(find_dotenv())\n",
    "   GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional libraries\n",
    "if COLAB:\n",
    "  !pip install -qU chromadb\n",
    "    \n",
    "# Import additional libraries\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of ressources\n",
    "if COLAB:\n",
    "    # Clone the data repository into colab\n",
    "    !git clone https://github.com/openknowledge/workshop-genai-camp-data.git\n",
    "    %cd workshop-genai-camp-data\n",
    "    !git lfs pull \n",
    "    ROOT_PATH = \"/content/workshop-genai-camp-data/day-03\"\n",
    "else:\n",
    "    ROOT_PATH = \"..\"\n",
    "DATA_PATH = ROOT_PATH + \"/data\"\n",
    "EVALUATION_PATH = ROOT_PATH + \"/evaluation\"\n",
    "KNOWLEDGEBASE_PATH = ROOT_PATH + \"/knowledgebase\"\n",
    "BOOK_CATALOG_FILE = DATA_PATH + \"/books.json\"\n",
    "TESTDATA_FILE = EVALUATION_PATH + \"/synthetic_testset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default models\n",
    "GENERATION_MODEL = \"gemini-1.5-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
    "\n",
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9 \n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200 \n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT = \" \"\n",
    "\n",
    "# Set defaults for rag\n",
    "DEFAULT_K = 3\n",
    "DEFAULT_CHUNK_SIZE = 2000\n",
    "DEFAULT_CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MimeType(Enum):\n",
    "    \"\"\"\n",
    "    Enum for MIME types.\n",
    "    \"\"\"\n",
    "    JSON = \"application/json\"\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"\n",
    "    Response format model for Gemini API.\n",
    "    \"\"\"\n",
    "    response_mime_type: MimeType\n",
    "    response_schema: type\n",
    "    \n",
    "   \n",
    "def generate_gemini_completion(\n",
    "        user_prompt : str,\n",
    "        response_format: ResponseFormat | None = None,\n",
    "        system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n",
    "        model_name: str = GENERATION_MODEL, \n",
    "        verbose: bool = False\n",
    "        ) -> str: \n",
    "    \"\"\"\n",
    "    Call the GenAI model with function declarations and return the response.\n",
    "    Args:\n",
    "        user_prompt (str): The prompt to send to the model.\n",
    "        response_format (ResponseFormat): The format of the response.\n",
    "        system_prompt (str): The system prompt to use.\n",
    "        model_name (str): The name of the model to use.\n",
    "        verbose (bool): If True, print the response.\n",
    "    Returns:\n",
    "        str: The response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Configure response format\n",
    "    response_schema = None\n",
    "    response_mime_type = None\n",
    "    if response_format:\n",
    "        response_schema = response_format.response_schema\n",
    "        response_mime_type = response_format.response_mime_type.value\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        system_instruction=system_prompt,\n",
    "        response_schema=response_schema,\n",
    "        response_mime_type=response_mime_type,\n",
    "    )\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=user_prompt,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Response: {response}\")\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read objects from a JSON file\n",
    "def read_objects_from_json(file_path: str, cls: BaseModel) -> list:\n",
    "    \"\"\"Reads list of objects from a JSON file and returns the list.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        objects = [cls(**item) for item in data]\n",
    "    return objects\n",
    "\n",
    "# Define classes used in the ingestion process\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"Represents the metadata of a document which is stored in the knowledgebase.\"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    pub_year: int\n",
    "\n",
    "class Book(BaseModel):\n",
    "    \"\"\"Represents a book with its metadata.\"\"\"\n",
    "    metadata: Metadata\n",
    "    summary: str    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cfda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG building blocks\n",
    "\n",
    "# This will be the chromadb collection we use as a knowledge base. We do not need the client.\n",
    "chromadb_collection = PersistentClient(path=KNOWLEDGEBASE_PATH).get_or_create_collection(name=\"default\")\n",
    "\n",
    "class FetchedChunk(BaseModel):\n",
    "    \"\"\"Represents a chunk fetched from the knowledgebase.\"\"\"\n",
    "    chunk: str\n",
    "    metadata: Metadata\n",
    "\n",
    "# Building Block \"Embedding\": Create multi dimensional embeddings for a given chunk.\n",
    "def do_embed(chunk: str) -> list[float]:\n",
    "  \"\"\" Embeds a given chunk and returns the embedding\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunk : str\n",
    "      The chunk to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embedding: [float]\n",
    "      The created embedding\n",
    "  \"\"\"\n",
    "  content_embeddings = client.models.embed_content(model=EMBEDDING_MODEL, contents=chunk).embeddings\n",
    "  return content_embeddings[0].values\n",
    "\n",
    "\n",
    "# Building Block \"Augmentation\": Create an updated prompt by merging the original user input with the provided context\n",
    "# Attention: We manipulated the augmented prompt in order to see the guardrails in action\n",
    "def augment(user_input: str, context: list[str]) -> str:\n",
    "  \"\"\" Augments a given user input by merging it with the provided context and returns the augmented prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be augmented\n",
    "  context : [str]\n",
    "      The context to be merged with the user input\n",
    "  Returns\n",
    "  -------\n",
    "  augmented_prompt: str\n",
    "      The created augmented prompt\n",
    "  \"\"\"\n",
    "  prepared_context = \"\\n\".join(context)\n",
    "  augmented_prompt = f\"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n{prepared_context}?\\n\n",
    "    Question: \\n{user_input}\\n\n",
    "\n",
    "    Answer:\n",
    "  \"\"\"\n",
    "  return augmented_prompt\n",
    "\n",
    "# Building Block \"Top-k Fetching\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "def do_top_k_fetching(user_input_embedding: list[float], top_k: int) -> list[FetchedChunk]:\n",
    "  \"\"\" Fetches the k semantically closest chunks to the user input from the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input_embedding : [float]\n",
    "      The embedding of the user input\n",
    "  top_k : int\n",
    "      The number of semantically closest chunks to be fetched\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Since we will do the fetching always only for one user_input,\n",
    "  # instead of querying for multiple embeddings simultanously as allowed by the choma API,\n",
    "  # we add the embeddings below to a list and return only the first document (chunk)\n",
    "  \n",
    "  query_result = chromadb_collection.query(\n",
    "      query_embeddings=[user_input_embedding],\n",
    "      n_results=top_k,\n",
    "  )\n",
    "  chunks = query_result[\"documents\"][0]\n",
    "  metadatas = query_result[\"metadatas\"][0]\n",
    "  \n",
    "  fetched_chunks = []\n",
    "  for i in range(len(chunks)):\n",
    "    chunk = chunks[i]\n",
    "    metadata = metadatas[i]\n",
    "    fetched_chunk = FetchedChunk(chunk=chunk, metadata=Metadata(**metadata))\n",
    "    fetched_chunks.append(fetched_chunk)\n",
    "  return fetched_chunks\n",
    "\n",
    "# Building Block \"Generation\": Use the generation model to create a response\n",
    "def generate_response(prompt: str) -> str:\n",
    "  \"\"\" Generates a response for a given prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  prompt : str\n",
    "      The prompt to be used for the generation\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  \"\"\"\n",
    "  return generate_gemini_completion(\n",
    "      model_name=GENERATION_MODEL,\n",
    "      user_prompt=prompt,\n",
    "  )\n",
    "\n",
    "# The rag function should now return the response and the context in order to be evaluated further\n",
    "def do_rag(user_input: str, top_k: int, retrieval_only: bool = False) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(chunk=user_input)\n",
    "\n",
    "  # \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  fetched_chunks = do_top_k_fetching(user_input_embedding=user_input_embedding, top_k=top_k)\n",
    "  context = [chunk.chunk for chunk in fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  if not retrieval_only:\n",
    "    # Generate a response using the augmented prompt\n",
    "    response = generate_response(prompt=augmented_prompt)\n",
    "  else:\n",
    "    response = \"No response generated. Only retrieval was requested.\"\n",
    "\n",
    "  return (response, fetched_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aaee06",
   "metadata": {},
   "source": [
    "### Exercise 01: Synthetic Testset\n",
    "RAG evaluation is key for high performing genai systems. First priority should be to gather testsets from human experts. Additionally, it is possible to create a synthetic testset.  \n",
    "In this exercise you will create a synthetic testset using genai. For this, use your prompt engineering skills to generate interactions from possible customers with bookstore employees. The customer should ask for a book by recalling some details of the specific book without mentioning the title. The employee should respond with the respective book title. Here is a possible interaction:  \n",
    "\n",
    "**Customer**: *\"I’m looking for an old book about a scientist who creates a creature out of dead body parts! The creature comes to life and things go very badly.\"*  \n",
    "\n",
    "**Employee**: *\"You’re talking about Frankenstein; or, The Modern Prometheus by Mary Shelley. The scientist is named Victor Frankenstein, and the creature doesn’t actually have a name, though people often mix them up.\"*\n",
    "  \n",
    "Your task is to provide the system prompt, which should lead to the generation of a possible customer question.  \n",
    "In addition, you should provide the prompt, which adds the neccessary information to the generation of the employee response. Analyse the resulting interactions and improve your prompts based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be one interaction between the user and the employee.\n",
    "class TestdataItem(BaseModel):\n",
    "    \"\"\"Represents a test data item with its input, output, and source.\"\"\"\n",
    "    input: str  # Customer's question\n",
    "    output: str # Employee's response\n",
    "    source: Book # Book which the employee remembers the answer from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27076b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your prompt engineering skills to create a system prompt for generating possible customer questions.\n",
    "# Define the role of the assistant as an assistant for test data generation.\n",
    "# Give context: The assistant will be provided with a summary of a book.\n",
    "# Define the task: The assistant should pretend to be a customer in a book store, who looks for the book described by the given summary.\n",
    "# Define the output: The customer should vaguely remember key points of the book, but not the finer details and not the title.\n",
    "# Provide an example of the CONTEXT and CUSTOMER. For this, use the CONTEXT and CUSTOMER part from the prompt below.\n",
    "GEMINI_CUSTOMER_SYSTEM_PROMPT = \"TODO\"\n",
    "    \n",
    "# This is the system prompt for generating the employee response.\n",
    "GEMINI_BOOKSTORE_EMPLOYEE_SYSTEM_PROMPT = \\\n",
    "    \"You are a book store employee. You are talking to a customer, who is looking for a book. \" \\\n",
    "    \"The customer is not sure about the title, but remembers some details from the book. \" \\\n",
    "    \"You should just provide, how the book store employee would respond. \" \\\n",
    "    \"The employee should only use the book described in the CONTEXT to answer the customer. \" \\\n",
    "    \"The employee should always name the title of the book and respond in a maximum number of two sentences.\" \\\n",
    "    \"Here is an Example:\" \\\n",
    "    \"CUSTOMER: I’m looking for an old book about a scientist who creates a creature out of dead body parts? The creature comes to life and things go very badly.\" \\\n",
    "    \"CONTEXT: \\\"Frankenstein; Or, The Modern Prometheus\\\" by Mary Wollstonecraft Shelley is a novel written in the early 19th century. The story explores themes of ambition, the quest for knowledge, and the consequences of man's hubris through the experiences of Victor Frankenstein and the monstrous creation of his own making.   The opening of the book introduces Robert Walton, an ambitious explorer on a quest to discover new lands and knowledge in the icy regions of the Arctic. In his letters to his sister Margaret, he expresses both enthusiasm and the fear of isolation in his grand venture. As Walton's expedition progresses, he encounters a mysterious, emaciated stranger who has faced great suffering—furthering the intrigue of his narrative. This stranger ultimately reveals his tale of creation, loss, and the profound consequences of seeking knowledge that lies beyond human bounds. The narrative is set up in a manner that suggests a deep examination of the emotions and ethical dilemmas faced by those who dare to defy the natural order. (This is an automatically generated summary.)\\\"\" \\\n",
    "    \"BOOK STORE EMPLOYEE: You’re talking about Frankenstein; or, The Modern Prometheus by Mary Shelley. The scientist is named Victor Frankenstein, and the creature doesn’t actually have a name, though people often mix them up. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4057a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define response formats for the customer question and the bookstore employee response\n",
    "class CustomerQuestion(BaseModel):\n",
    "    \"\"\"Defines the question of a customer looking for a book.\"\"\"\n",
    "    customer_question: str\n",
    "\n",
    "class BookstoreEmployeeResponse(BaseModel):\n",
    "    \"\"\"Defines the response of a bookstore employee to a customer question.\"\"\"\n",
    "    employee_response: str\n",
    "\n",
    "response_format_customer_question = ResponseFormat(\n",
    "    response_mime_type=MimeType.JSON,\n",
    "    response_schema=CustomerQuestion\n",
    ")\n",
    "\n",
    "response_format_bookstore_employee_response = ResponseFormat(\n",
    "    response_mime_type=MimeType.JSON,\n",
    "    response_schema=BookstoreEmployeeResponse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4895351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of test data items to be generated. Since this is only for demonstration purposes, we will only generate a few items.\n",
    "n_testset_items = 2\n",
    "\n",
    "# Get some random books from the book catalog and generate test data items\n",
    "books = read_objects_from_json(file_path=BOOK_CATALOG_FILE, cls=Book)\n",
    "random.shuffle(books)\n",
    "\n",
    "# List for storing the test data items\n",
    "testdata_items: list[TestdataItem] = []\n",
    "\n",
    "for book in books:\n",
    "    # Define the input prompt for the customer question, generate response and validate it\n",
    "    input_prompt = f\"SUMMARY: {book.summary}\\nCUSTOMER:\"\n",
    "    customer_input_response = generate_gemini_completion(system_prompt=GEMINI_CUSTOMER_SYSTEM_PROMPT, user_prompt=input_prompt, response_format=response_format_customer_question)\n",
    "    customer_input = CustomerQuestion(**json.loads(customer_input_response))\n",
    "\n",
    "    # TODO: Define the employee prompt. Look at the example above for formatting.\n",
    "    # Look and at the system prompt for the employee for what variables should be provided to the prompt.\n",
    "    employee_prompt = \"\"\n",
    "    employee_output_response = generate_gemini_completion(system_prompt=GEMINI_BOOKSTORE_EMPLOYEE_SYSTEM_PROMPT, user_prompt=employee_prompt, response_format=response_format_bookstore_employee_response)\n",
    "    employee_output = BookstoreEmployeeResponse(**json.loads(employee_output_response))\n",
    "    \n",
    "    # Create a test data item and add it to the list\n",
    "    testdata_item = TestdataItem(\n",
    "        input=customer_input.customer_question,\n",
    "        output=employee_output.employee_response,\n",
    "        source=book\n",
    "    )\n",
    "    testdata_items.append(testdata_item)\n",
    "\n",
    "    # Check if we have reached the desired number of test data items\n",
    "    if len(testdata_items) >= n_testset_items:\n",
    "        break\n",
    "\n",
    "testdata_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c98f19",
   "metadata": {},
   "source": [
    "### Exercise 02: Evaluate the system\n",
    "Your task is to implement another metric:  \n",
    "1. **Precision**: Precision at K is the ratio of correctly identified relevant items within the total recommended items inside the K-long list. Simply put, it shows how many ground truth elements are included in the retrieved documents.\n",
    "\n",
    "See [here](https://www.evidentlyai.com/ranking-metrics/precision-recall-at-k) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb49e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load generated test data items\n",
    "testdata_items = read_objects_from_json(file_path=TESTDATA_FILE, cls=TestdataItem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7befbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the hit rate. \n",
    "def calculate_hit_rate(ground_truth, retrieved: list[str]):\n",
    "    gt_set = set(ground_truth)\n",
    "    retrieved_set = set(retrieved)\n",
    "    return int(bool(gt_set & retrieved_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196943fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the calculate_reciprocal_rank. \n",
    "def calculate_reciprocal_rank(ground_truth: list[str], retrieved: list[str]) -> float:\n",
    "    gt_set = set(ground_truth)\n",
    "    for rank, item in enumerate(retrieved, start=1):\n",
    "        if item in gt_set:\n",
    "            return 1 / rank\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the function to calculate the precision.\n",
    "def calculate_precision(ground_truth: list[str], retrieved: list[str]) -> float:\n",
    "\n",
    "    # TODO: Get the number (length) of retrieved items\n",
    "    k = \n",
    "\n",
    "    # Elements, which are present in both lists\n",
    "    relevant_items_in_retrieved = (set(ground_truth) & set(retrieved))\n",
    "\n",
    "    # TODO: Return the number of relevant items (ground truth) in the retrieved list divided by the number of retrieved items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the metric functions\n",
    "ground_truth = [\"The Adventures of Tom Sawyer\"]\n",
    "retrieved = [\"Huckleberry Finn\", \"The Adventures of Tom Sawyer\", \"The Great Gatsby\"]\n",
    "\n",
    "assert calculate_precision(ground_truth, retrieved) == 1/3\n",
    "assert calculate_reciprocal_rank(ground_truth, retrieved) == 1/2\n",
    "assert calculate_hit_rate(ground_truth, retrieved) == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a961140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "# In a real-world scenario, you would use a larger test set and more sophisticated evaluation metrics.\n",
    "# Here, we only evaluate the retrieval, since this is crucial for the RAG pipeline and does not necessarily depend on a generation model.\n",
    "# For this example, we will just use the hit rate, reciprocal rank, and precision.\n",
    "\n",
    "# Initialize lists to store the metrics\n",
    "hit_rates = []\n",
    "reciprocal_ranks = []\n",
    "precisions = []\n",
    "\n",
    "# Set the number of top k items to be retrieved\n",
    "top_k = 3\n",
    "\n",
    "# Iterate over the test data items\n",
    "for testdata_item in testdata_items:\n",
    "\n",
    "    # Prepare the ground truth\n",
    "    ground_truth = [testdata_item.source.metadata.title]\n",
    "\n",
    "    # Run the RAG pipeline. We only need the retrieval part for evaluation.\n",
    "    response, fetched_chunks = do_rag(user_input=testdata_item.input, top_k=top_k, retrieval_only=True)\n",
    "\n",
    "    # Check if the ground truth is in the context\n",
    "    retrieved = [item.metadata.title for item in fetched_chunks]\n",
    "\n",
    "    # Calculate metrics\n",
    "    # Hit rate\n",
    "    hit_rate_value = calculate_hit_rate(ground_truth=ground_truth, retrieved=retrieved)\n",
    "    hit_rates.append(hit_rate_value)\n",
    "    \n",
    "    # Reciprocal rank\n",
    "    reciprocal_rank = calculate_reciprocal_rank(ground_truth=ground_truth, retrieved=retrieved)\n",
    "    reciprocal_ranks.append(reciprocal_rank)\n",
    "\n",
    "    # Precision\n",
    "    precision = calculate_precision(ground_truth=ground_truth, retrieved=retrieved)\n",
    "    precisions.append(precision)\n",
    "\n",
    "mean_hit_rate = sum(hit_rates) / len(hit_rates)\n",
    "print(f\"Mean hit rate: {mean_hit_rate:.2f}\")\n",
    "\n",
    "mean_reciprocal_rank = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "print(f\"Mean reciprocal rank: {mean_reciprocal_rank:.2f}\")\n",
    "\n",
    "mean_precision = sum(precisions) / len(precisions)\n",
    "print(f\"Mean precision: {mean_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a293a6f",
   "metadata": {},
   "source": [
    "### Exercise 03: Visualize performance of different configurations\n",
    "In this exercise you should visualize the metrics above for different *top_k* and interpret the results. Create a [plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html), showing the values for Mean Hit Rate, MRR and Mean Precision for *k* ranging from 1 to 10.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for storing the evaluation result\n",
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"Represents the evaluation result with its metrics.\"\"\"\n",
    "    mean_hit_rate: float\n",
    "    mean_reciprocal_rank: float\n",
    "    mean_precision: float\n",
    "\n",
    "# Function to run the evaluation and return the metrics\n",
    "def run_evaluation(top_k: int) -> EvaluationResult:\n",
    "    hit_rates = []\n",
    "    reciprocal_ranks = []\n",
    "    precisions = []\n",
    "\n",
    "    for testdata_item in testdata_items:\n",
    "\n",
    "        # Prepare the ground truth\n",
    "        ground_truth = [testdata_item.source.metadata.title]\n",
    "\n",
    "        # Run the RAG pipeline\n",
    "        _, fetched_chunks = do_rag(user_input=testdata_item.input, top_k=top_k, retrieval_only=True)\n",
    "\n",
    "        # Check if the ground truth is in the context\n",
    "        retrieved = [item.metadata.title for item in fetched_chunks]\n",
    "\n",
    "        # Calculate metrics\n",
    "        # Hit rate\n",
    "        hit_rate_value = calculate_hit_rate(ground_truth=ground_truth, retrieved=retrieved)\n",
    "        hit_rates.append(hit_rate_value)\n",
    "        \n",
    "        # Reciprocal rank\n",
    "        reciprocal_rank = calculate_reciprocal_rank(ground_truth=ground_truth, retrieved=retrieved)\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "\n",
    "        # Precision\n",
    "        precision = calculate_precision(ground_truth=ground_truth, retrieved=retrieved)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    # Calculate mean values\n",
    "    mean_hit_rate = sum(hit_rates) / len(hit_rates)\n",
    "    mean_reciprocal_rank = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    mean_precision = sum(precisions) / len(precisions)\n",
    "    return EvaluationResult(\n",
    "        mean_hit_rate=mean_hit_rate,\n",
    "        mean_reciprocal_rank=mean_reciprocal_rank,\n",
    "        mean_precision=mean_precision\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd971ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation for k=1 to k=10 with a step of 2 (to speed up the process for demonstration purposes)\n",
    "# This takes approximately 3.5 minutes.\n",
    "k_values = list(range(1, 11,2))\n",
    "\n",
    "# Initialize lists to store results\n",
    "mean_hit_rates = []\n",
    "mean_reciprocal_ranks = []\n",
    "mean_precisions = []\n",
    "\n",
    "# Iterate over the k values and run the evaluation\n",
    "for k in k_values:\n",
    "    result = run_evaluation(top_k=k)\n",
    "    mean_hit_rates.append(result.mean_hit_rate)\n",
    "    mean_reciprocal_ranks.append(result.mean_reciprocal_rank)\n",
    "    mean_precisions.append(result.mean_precision)\n",
    "    sleep(30)  # Sleep to avoid rate limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the evaluation results (mean_hit_rates, mean_reciprocal_ranks, mean_precisions)\n",
    "# in a line plot for different k. What do you observe?\n",
    "plt.figure(figsize=(10, 6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
