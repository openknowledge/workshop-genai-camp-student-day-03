{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePcF01H6rr16"
   },
   "source": [
    "# GenAI-Camp: Day 03\n",
    "## Lesson: Basic RAG\n",
    "\n",
    "This lesson is intended to show you the basics of a Retrieval Augmented Generation (RAG) system.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- implement the different building blocks of RAG\n",
    "- create an ingestion pipeline from the building blocks\n",
    "- create a retrieval pipeline from the building blocks\n",
    "- use a RAG system to generate responses to user inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxQu4eGGDrJe"
   },
   "source": [
    "### Set up the environment\n",
    "Import the necessary libraries, set constants, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from pydantic import BaseModel\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J595XWK4AuQ-",
    "outputId": "a1938bb5-1455-4b38-9046-d34483775689"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   from google.colab import userdata\n",
    "   GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\")\n",
    "else:\n",
    "   from dotenv import load_dotenv, find_dotenv\n",
    "   load_dotenv(find_dotenv())\n",
    "   GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFEsl___H2n4"
   },
   "outputs": [],
   "source": [
    "# Install additional libraries\n",
    "if COLAB:\n",
    "  !pip install -qU langchain-text-splitters chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3UN1LX-IEcp"
   },
   "outputs": [],
   "source": [
    "# Import additional libraries\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from chromadb import EphemeralClient\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of ressources\n",
    "if COLAB:\n",
    "    # Clone the data repository into colab\n",
    "    !git clone https://github.com/openknowledge/workshop-genai-camp-data.git\n",
    "    ROOT_PATH = \"/content/workshop-genai-camp-data/day-03\"\n",
    "else:\n",
    "    ROOT_PATH = \"..\"\n",
    "DATA_PATH = ROOT_PATH + \"/data\"\n",
    "KNOWLEDGEBASE_PATH = ROOT_PATH + \"/knowledgebase\"\n",
    "BOOK_CATALOG_FILE = DATA_PATH + \"/books.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKG1EPEGdvvC"
   },
   "outputs": [],
   "source": [
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_MODEL = \"gemini-1.5-flash\"\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9\n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200\n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJW72wGgPOxt"
   },
   "outputs": [],
   "source": [
    "# This will be the chromadb collection we use as a knowledge base. We do not need the in-memory EphemeralClient.\n",
    "chromadb_collection =  EphemeralClient().get_or_create_collection(name=\"default\")\n",
    "\n",
    "# Have a look into the knowledgebase\n",
    "def peek_knowledgebase():\n",
    "  \"\"\"Shows the first ten items of the knowledgebase\"\"\"\n",
    "  print(chromadb_collection.peek())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s8AAOATdqCa"
   },
   "outputs": [],
   "source": [
    "# Function to generate a completion with the Gemini model\n",
    "def generate_gemini_completion(\n",
    "        model_name: str = DEFAULT_MODEL, \n",
    "        temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "        top_k: int = DEFAULT_CONFIG_TOP_K, \n",
    "        max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS, \n",
    "        system_prompt : str = DEFAULT_SYSTEM_PROMPT, \n",
    "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "        verbose: bool = False\n",
    "        ) -> str: \n",
    "    \n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_GEMINI_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False            \n",
    "    Returns \n",
    "    -------\n",
    "    str :\n",
    "        the generated text      \n",
    "    \"\"\"    \n",
    "    if verbose: \n",
    "        # print out summary of input values / parameters\n",
    "        print(f'Generating answer for following config:')\n",
    "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
    "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
    "        print(f'  - MODEL used:\\n {model_name} (temperature = {temperature}, top_k = {top_k}, max_output_tokens = {max_output_tokens})')\n",
    "\n",
    "    # create generation config \n",
    "    model_config = types.GenerateContentConfig(\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        system_instruction=system_prompt,\n",
    "    )\n",
    "    \n",
    "    # create generation request\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=user_prompt,\n",
    "        config=model_config,\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read objects from a JSON file\n",
    "def read_objects_from_json(file_path: str, cls: BaseModel) -> list:\n",
    "    \"\"\"Reads list of objects from a JSON file and returns the list.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        objects = [cls(**item) for item in data]\n",
    "    return objects\n",
    "\n",
    "# Define classes used in the ingestion process\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"Represents the metadata of a document which is stored in the knowledgebase.\"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    pub_year: int\n",
    "\n",
    "class Book(BaseModel):\n",
    "    \"\"\"Represents a book with its metadata.\"\"\"\n",
    "    metadata: Metadata\n",
    "    summary: str   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqcAgsAzEl2A"
   },
   "source": [
    "### Configure the genAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWcu0XZgEucE"
   },
   "outputs": [],
   "source": [
    "GENERATION_MODEL = \"gemini-1.5-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EigTzQ_eAWd1"
   },
   "source": [
    "### Configure retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeNQxSjSALrG"
   },
   "outputs": [],
   "source": [
    "DEFAULT_K = 3\n",
    "DEFAULT_CHUNK_SIZE = 2000\n",
    "DEFAULT_CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd-X4JzHCBui"
   },
   "source": [
    "### Define RAG Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BquBXbJxBYWd"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Chunking\": Split the content into smaller chunks\n",
    "def do_chunk(text: str, chunk_size: int, chunk_overlap: int) -> list[str]:\n",
    "  \"\"\" Chunks a given text by a given chunk size and chunk overlap and returns a list of chunks\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  text : str\n",
    "      The text to be chunked\n",
    "  chunk_size : int, optional [default: DEFAULT_CHUNK_SIZE]\n",
    "        The desired chunk size\n",
    "  chunk_overlap : int, optional [default: DEFAULT_CHUNK_OVERLAP]\n",
    "        The desired chunk overlap\n",
    "  Returns\n",
    "  -------\n",
    "  chunks: [str]\n",
    "      The created chunks\n",
    "  \"\"\"\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=chunk_size,\n",
    "      chunk_overlap=chunk_overlap,\n",
    "      length_function=len,\n",
    "  )\n",
    "  return text_splitter.split_text(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cuFXzCyBxdU"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Embedding\": Create multi dimensional embeddings for a given chunk.\n",
    "def do_embed(chunk: str) -> list[float]:\n",
    "  \"\"\" Embeds a given chunk and returns the embedding\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunk : str\n",
    "      The chunk to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embedding: [float]\n",
    "      The created embedding\n",
    "  \"\"\"\n",
    "  content_embeddings = client.models.embed_content(model=EMBEDDING_MODEL, contents=chunk).embeddings\n",
    "  return content_embeddings[0].values\n",
    "\n",
    "def do_batch_embed(chunks: list[str]) -> list[list[float]]:\n",
    "  \"\"\" Embeds a list of chunks and returns the embeddings\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunks : [str]\n",
    "      The chunks to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embeddings: [list[float]]\n",
    "      The created embeddings\n",
    "  \"\"\"\n",
    "  content_embeddings = client.models.embed_content(model=EMBEDDING_MODEL, contents=chunks).embeddings\n",
    "  return [content_embedding.values for content_embedding in content_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpJduX2pSzxi"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Knowledgebase\": Store embeddings and the corresponding content in a vectorstore\n",
    "def persist_embeddings(chunks: list[str], embeddings: list[list[float]], metadatas: list[dict])-> None:\n",
    "  \"\"\" Persists the embeddings and the chunks in the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunks : [str]\n",
    "      The chunks to be persisted\n",
    "  embeddings: [list[float]]\n",
    "      The corresponding embeddings to be persisted\n",
    "  \"\"\"\n",
    "  ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "  chromadb_collection.add(ids=ids, documents=chunks, embeddings=embeddings, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fyut_CCPC37U"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Augmentation\": Create an updated prompt by merging the original user input with the provided context\n",
    "def augment(user_input: str, context: list[str]) -> str:\n",
    "  \"\"\" Augments a given user input by merging it with the provided context and returns the augmented prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be augmented\n",
    "  context : [str]\n",
    "      The context to be merged with the user input\n",
    "  Returns\n",
    "  -------\n",
    "  augmented_prompt: str\n",
    "      The created augmented prompt\n",
    "  \"\"\"\n",
    "  prepared_context = \"\\n\".join(context)\n",
    "  augmented_prompt = f\"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n{prepared_context}?\\n\n",
    "    Question: \\n{user_input}\\n\n",
    "\n",
    "    Answer:\n",
    "  \"\"\"\n",
    "  return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKHUg84XDLRp"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Top-k Fetching\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "def do_top_k_fetching(user_input_embedding: list[float], top_k: int) -> list[str]:\n",
    "  \"\"\" Fetches the k semantically closest chunks to the user input from the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input_embedding : [float]\n",
    "      The embedding of the user input\n",
    "  top_k : int\n",
    "      The number of semantically closest chunks to be fetched\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Since we will do the fetching always only for one user_input,\n",
    "  # instead of querying for multiple embeddings simultanously as allowed by the choma API,\n",
    "  # we add the embeddings below to a list and return only the first document (chunk)\n",
    "  return chromadb_collection.query(\n",
    "      query_embeddings=[user_input_embedding],\n",
    "      n_results=top_k,\n",
    "  )[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARQ8Pxo1d7kq"
   },
   "outputs": [],
   "source": [
    "# Building Block \"Generation\": Use the generation model to create a response\n",
    "def generate_response(prompt: str) -> str:\n",
    "  \"\"\" Generates a response for a given prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  prompt : str\n",
    "      The prompt to be used for the generation\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  \"\"\"\n",
    "  response = generate_gemini_completion(\n",
    "      model_name=GENERATION_MODEL,\n",
    "      user_prompt=prompt,\n",
    "  )\n",
    "  print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7A5e_dY-chC"
   },
   "source": [
    "### Create the ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HalhxNB8-chE"
   },
   "outputs": [],
   "source": [
    "# This function defines the ingestion process for the book catalog\n",
    "def do_ingestion(book_catalog_file: str, chunk_size: int, chunk_overlap: int, batch_size: int = 100) -> None:\n",
    "  \"\"\" Ingests a list of files by a given file name\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  file_names : [str]\n",
    "      The names of the files to be ingested\n",
    "  \"\"\"\n",
    "  books = read_objects_from_json(file_path=book_catalog_file, cls=Book)\n",
    "\n",
    "  all_chunks = []  # Collect all chunks\n",
    "  all_metadatas = []  # Collect all metadatas\n",
    "\n",
    "  # Iterate over all books\n",
    "  for book in books:\n",
    "    # Load prepared book content\n",
    "    text_content = book.summary\n",
    "\n",
    "    # Chunk the content into smaller chunks\n",
    "    chunks = do_chunk(text_content, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    # Add the chunks and their corresponding metadata to the lists\n",
    "    # Note: We use the book.metadata for each chunk, assuming that the metadata is the same for all chunks of a book\n",
    "    all_chunks.extend(chunks)\n",
    "    all_metadatas.extend([book.metadata] * len(chunks))  # Create a list of metadatas for each chunk\n",
    "\n",
    "  # Process chunks in batches. Otherwise, we might run into quota issues.\n",
    "  for i in range(0, len(all_chunks), batch_size):\n",
    "\n",
    "    # Get the current batch of chunks and their corresponding metadata\n",
    "    chunk_batch = all_chunks[i:i + batch_size]\n",
    "    metadata_batch = all_metadatas[i:i + batch_size]\n",
    "    metadatas = [metadata.model_dump() for metadata in metadata_batch]  # Convert Metadata objects to dictionaries\n",
    "\n",
    "    # Embed the batch of chunks\n",
    "    embeddings = do_batch_embed(chunk_batch)\n",
    "\n",
    "    # Persist the embeddings and the chunks in the knowledgebase\n",
    "    persist_embeddings(chunk_batch, embeddings, metadatas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u56sn0894D4"
   },
   "source": [
    "### Perform ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GC2yAP9QWN2x"
   },
   "outputs": [],
   "source": [
    "# Perform ingestion. Depending on the chunk_size this might take some minutes.\n",
    "do_ingestion(book_catalog_file=BOOK_CATALOG_FILE, chunk_size=DEFAULT_CHUNK_SIZE, chunk_overlap=DEFAULT_CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DaQ8u7tj_UVG",
    "outputId": "931992f3-a85a-49ef-e614-1e82728be750"
   },
   "outputs": [],
   "source": [
    "# Use helper function to peek into knowledgebase\n",
    "peek_knowledgebase()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAKlDC8WclSD"
   },
   "source": [
    "### Exercise 01: Create RAG pipeline\n",
    "In this exercise you will create a rag pipeline for retrieving relevant chunks and generating a grounded response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tehtlj-T_wDc"
   },
   "outputs": [],
   "source": [
    "# TODO: Update the following function to perfom the retrieval pipeline\n",
    "def do_rag(user_input: str, verbose: bool = False) -> None:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and prints the response\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  verbose : bool, optional [default: False]\n",
    "      Whether to print details of the RAG process or not. Defaults to False\n",
    "  \"\"\"\n",
    "  # TODO: Embed the user input\n",
    "  user_input_embedding = \n",
    "\n",
    "  # TODO: \"R\" like \"Retrieval\": Get the top-k semantically closest chunks to the user input from the knowledgebase\n",
    "  context = \n",
    "  if verbose:\n",
    "    print(f'Retrieved context:\\n {context}')\n",
    "\n",
    "  # TODO: \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = \n",
    "  if verbose:\n",
    "    print(f'Augmented prompt:\\n {augmented_prompt}')\n",
    "\n",
    "  # TODO: \"G\" like \"Generation\": Generate a response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "BeRJbjeZcn-i",
    "outputId": "3162dc03-69a1-4313-b1ff-dbbbef8abce3"
   },
   "outputs": [],
   "source": [
    "# Define user input. This should be a question regarding one ingested book\n",
    "user_input= \"Hey, I'm looking for a novel about a young boy at the Mississippi river. I think the boy is named Huck?\"\n",
    "\n",
    "# TODO: Perform retrieval by executing the do_rag function"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
