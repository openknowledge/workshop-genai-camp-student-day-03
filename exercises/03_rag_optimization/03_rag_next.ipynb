{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ba3deb",
   "metadata": {},
   "source": [
    "# GenAI-Camp: Day 03\n",
    "## Lesson: Next RAG\n",
    "\n",
    "This lesson is intended to show you how to further extend a RAG system.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- use a fact-checking guardrail\n",
    "- use rag as a tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac7e55",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "Import the necessary libraries, set constants, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   from google.colab import userdata\n",
    "   GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\")\n",
    "else:\n",
    "   from dotenv import load_dotenv, find_dotenv\n",
    "   load_dotenv(find_dotenv())\n",
    "   GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "   JINA_API_KEY = os.getenv(\"JINA_API_KEY\")\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional libraries\n",
    "if COLAB:\n",
    "  !pip install -qU chromadb\n",
    "    \n",
    "# Import additional libraries\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of ressources\n",
    "if COLAB:\n",
    "    # Clone the data repository into colab\n",
    "    !git clone https://github.com/openknowledge/workshop-genai-camp-data.git\n",
    "    %cd workshop-genai-camp-data\n",
    "    !git lfs pull \n",
    "    ROOT_PATH = \"/content/workshop-genai-camp-data/day-03\"\n",
    "else:\n",
    "    ROOT_PATH = \"..\"\n",
    "DATA_PATH = ROOT_PATH + \"/data\"\n",
    "EVALUATION_PATH = ROOT_PATH + \"/evaluation\"\n",
    "KNOWLEDGEBASE_PATH = ROOT_PATH + \"/knowledgebase\"\n",
    "BOOK_CATALOG_FILE = DATA_PATH + \"/books.json\"\n",
    "TESTDATA_FILE = EVALUATION_PATH + \"/synthetic_testset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default models\n",
    "GENERATION_MODEL = \"gemini-1.5-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
    "\n",
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9 \n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200 \n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT = \" \"\n",
    "\n",
    "# Set defaults for rag\n",
    "DEFAULT_K = 3\n",
    "DEFAULT_CHUNK_SIZE = 2000\n",
    "DEFAULT_CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_completion(\n",
    "        model_name: str = GENERATION_MODEL, \n",
    "        temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "        top_k: int = DEFAULT_CONFIG_TOP_K, \n",
    "        max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS, \n",
    "        system_prompt : str = DEFAULT_SYSTEM_PROMPT, \n",
    "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "        verbose: bool = False\n",
    "        ) -> str: \n",
    "    \n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_GEMINI_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False            \n",
    "    Returns \n",
    "    -------\n",
    "    str :\n",
    "        the generated text      \n",
    "    \"\"\"    \n",
    "    if verbose: \n",
    "        # print out summary of input values / parameters\n",
    "        print(f'Generating answer for following config:')\n",
    "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
    "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
    "        print(f'  - MODEL used:\\n {model_name} (temperature = {temperature}, top_k = {top_k}, max_output_tokens = {max_output_tokens})')\n",
    "\n",
    "    # create generation config \n",
    "    model_config = types.GenerateContentConfig(\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        system_instruction=system_prompt,\n",
    "    )\n",
    "    \n",
    "    # create generation request\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=user_prompt,\n",
    "        config=model_config,\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metadata(BaseModel):\n",
    "    \"\"\"Represents the metadata of a document which is stored in the knowledgebase.\"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    pub_year: int\n",
    "\n",
    "class Book(BaseModel):\n",
    "    \"\"\"Represents a book with its metadata.\"\"\"\n",
    "    metadata: Metadata\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cfda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG building blocks\n",
    "\n",
    "# This will be the chromadb collection we use as a knowledge base. We do not need the client.\n",
    "chromadb_collection = PersistentClient(path=KNOWLEDGEBASE_PATH).get_or_create_collection(name=\"default\")\n",
    "\n",
    "class FetchedChunk(BaseModel):\n",
    "    \"\"\"Represents a chunk fetched from the knowledgebase.\"\"\"\n",
    "    chunk: str\n",
    "    metadata: Metadata\n",
    "\n",
    "# Building Block \"Embedding\": Create multi dimensional embeddings for a given chunk.\n",
    "def do_embed(chunk: str) -> list[float]:\n",
    "  \"\"\" Embeds a given chunk and returns the embedding\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunk : str\n",
    "      The chunk to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embedding: [float]\n",
    "      The created embedding\n",
    "  \"\"\"\n",
    "  content_embeddings = client.models.embed_content(model=EMBEDDING_MODEL, contents=chunk).embeddings\n",
    "  return content_embeddings[0].values\n",
    "\n",
    "# Building Block \"Augmentation\": Create an updated prompt by merging the original user input with the provided context\n",
    "# Attention: We manipulated the augmented prompt in order to see the guardrails in action\n",
    "def augment(user_input: str, context: list[str]) -> str:\n",
    "  \"\"\" Augments a given user input by merging it with the provided context and returns the augmented prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be augmented\n",
    "  context : [str]\n",
    "      The context to be merged with the user input\n",
    "  Returns\n",
    "  -------\n",
    "  augmented_prompt: str\n",
    "      The created augmented prompt\n",
    "  \"\"\"\n",
    "  prepared_context = \"\\n\".join(context)\n",
    "  augmented_prompt = f\"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n{prepared_context}?\\n\n",
    "    Question: \\n{user_input}\\n\n",
    "\n",
    "    Answer:\n",
    "  \"\"\"\n",
    "  return augmented_prompt\n",
    "\n",
    "# Building Block \"Top-k Fetching\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "def do_top_k_fetching(user_input_embedding: list[float], top_k: int) -> list[FetchedChunk]:\n",
    "  \"\"\" Fetches the k semantically closest chunks to the user input from the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input_embedding : [float]\n",
    "      The embedding of the user input\n",
    "  top_k : int\n",
    "      The number of semantically closest chunks to be fetched\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Since we will do the fetching always only for one user_input,\n",
    "  # instead of querying for multiple embeddings simultanously as allowed by the choma API,\n",
    "  # we add the embeddings below to a list and return only the first document (chunk)\n",
    "  \n",
    "  query_result = chromadb_collection.query(\n",
    "      query_embeddings=[user_input_embedding],\n",
    "      n_results=top_k,\n",
    "  )\n",
    "  chunks = query_result[\"documents\"][0]\n",
    "  metadatas = query_result[\"metadatas\"][0]\n",
    "  \n",
    "  fetched_chunks = []\n",
    "  for i in range(len(chunks)):\n",
    "    chunk = chunks[i]\n",
    "    metadata = metadatas[i]\n",
    "    fetched_chunk = FetchedChunk(chunk=chunk, metadata=Metadata(**metadata))\n",
    "    fetched_chunks.append(fetched_chunk)\n",
    "  return fetched_chunks\n",
    "\n",
    "# Building Block \"Generation\": Use the generation model to create a response\n",
    "def generate_response(prompt: str) -> str:\n",
    "  \"\"\" Generates a response for a given prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  prompt : str\n",
    "      The prompt to be used for the generation\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  \"\"\"\n",
    "  return generate_gemini_completion(\n",
    "      model_name=GENERATION_MODEL,\n",
    "      user_prompt=prompt,\n",
    "  )\n",
    "\n",
    "# The rag function should now return the response and the context in order to be evaluated further\n",
    "def do_rag(user_input: str, top_k: int) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(chunk=user_input)\n",
    "\n",
    "  # \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  fetched_chunks = do_top_k_fetching(user_input_embedding=user_input_embedding, top_k=top_k)\n",
    "  context = [chunk.chunk for chunk in fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  response = generate_response(prompt=augmented_prompt)\n",
    "\n",
    "  return (response, fetched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ebdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classes and convenient functions for function calls\n",
    "class FunctionCall(BaseModel):\n",
    "    \"\"\"\n",
    "    Function call model for Gemini API.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    arguments: dict\n",
    "\n",
    "\n",
    "class GeminiResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Gemini response model.\n",
    "    \"\"\"\n",
    "    text: str | None\n",
    "    function_call: FunctionCall | None\n",
    "\n",
    "\n",
    "class MimeType(Enum):\n",
    "    \"\"\"\n",
    "    Enum for MIME types.\n",
    "    \"\"\"\n",
    "    JSON = \"application/json\"\n",
    "\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"\n",
    "    Response format model for Gemini API.\n",
    "    \"\"\"\n",
    "    response_mime_type: MimeType\n",
    "    response_schema: type\n",
    "    \n",
    "   \n",
    "def enhanced_generate_gemini_completion(\n",
    "        user_prompt : str,\n",
    "        response_format: ResponseFormat | None = None,\n",
    "        system_prompt: str = DEFAULT_SYSTEM_PROMPT,\n",
    "        model_name: str = GENERATION_MODEL, \n",
    "        function_declarations: list | None = None,\n",
    "        verbose: bool = False\n",
    "        ): \n",
    "    \"\"\"\n",
    "    Call the GenAI model with function declarations and return the response.\n",
    "    Args:\n",
    "        user_prompt (str): The prompt to send to the model.\n",
    "        response_format (ResponseFormat): The format of the response.\n",
    "        system_prompt (str): The system prompt to use.\n",
    "        model_name (str): The name of the model to use.\n",
    "        function_declarations (list): List of function declarations.\n",
    "        verbose (bool): If True, print the response.\n",
    "    Returns:\n",
    "        GeminiResponse: The response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Configure tools\n",
    "    tools = []\n",
    "    if function_declarations:\n",
    "        tools.append(types.Tool(function_declarations=function_declarations))\n",
    "\n",
    "    # Configure response format\n",
    "    response_schema = None\n",
    "    response_mime_type = None\n",
    "    if response_format:\n",
    "        response_schema = response_format.response_schema\n",
    "        response_mime_type = response_format.response_mime_type.value\n",
    "\n",
    "    config = types.GenerateContentConfig(\n",
    "        tools=tools,\n",
    "        system_instruction=system_prompt,\n",
    "        response_schema=response_schema,\n",
    "        response_mime_type=response_mime_type,\n",
    "    )\n",
    "\n",
    "    # Send request with function declarations\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=user_prompt,\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    function_call = None\n",
    "    if response.candidates[0].content.parts[0].function_call:\n",
    "        function_call_gemini = response.candidates[0].content.parts[0].function_call\n",
    "        function_call = FunctionCall(\n",
    "            name=function_call_gemini.name,\n",
    "            arguments=function_call_gemini.args,\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Response: {response}\")\n",
    "    \n",
    "    return GeminiResponse(\n",
    "        text=response.candidates[0].content.parts[0].text,\n",
    "        function_call=function_call,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e7684",
   "metadata": {},
   "source": [
    "### Exercise 01: Fact-Check\n",
    "Fact Checking is an important feature of RAG systems. Your task is to update the `do_rag` function in order to  check if the answer is grounded in the context. You should use a LLM-as-a-judge approach. For this use a separate call to the gemini model and create a prompt, which should check if the original answer is grounded in the retrieved context.  \n",
    "**Hints**:\n",
    "* You can use the enhaced gemini completion method, if you need response formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10396f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8f48e",
   "metadata": {},
   "source": [
    "### Exercise 02: RAG-as-a-tool\n",
    "RAG systems are useful for a lot of usecases, but sometimes the user just wants some general advice, which ist not based on internal knowldge. As a last exercise, you should provide the rag system as a tool to the model. Create a separate function, which encapsulates the RAG logic. Write a function declaration for this, and call the tool if neccessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a reminder, this is how you declare a function for tool calling:\n",
    "weather_function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get the current weather for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The location for which to get the weather.\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
