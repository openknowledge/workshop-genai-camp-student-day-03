{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ba3deb",
   "metadata": {},
   "source": [
    "# GenAI-Camp: Day 03\n",
    "## Lesson: RAG Optimization\n",
    "\n",
    "This lesson is intended to show you how to optimize a RAG system.\n",
    "\n",
    "During this lesson you will learn how to ...\n",
    "\n",
    "- use HYDE for generating hypothetical documents to potentially boost the retrieval\n",
    "- use a reranker to improve RAG performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ac7e55",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "Import the necessary libraries, set constants, and define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "   from google.colab import userdata\n",
    "   GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
    "   COLAB = True\n",
    "   print(\"Running on COLAB environment.\")\n",
    "else:\n",
    "   from dotenv import load_dotenv, find_dotenv\n",
    "   load_dotenv(find_dotenv())\n",
    "   GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "   JINA_API_KEY = os.getenv(\"JINA_API_KEY\")\n",
    "   COLAB = False\n",
    "   print(\"WARNING: Running on LOCAL environment.\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional libraries\n",
    "if COLAB:\n",
    "  !pip install -qU chromadb\n",
    "    \n",
    "# Import additional libraries\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of ressources\n",
    "if COLAB:\n",
    "    # Clone the data repository into colab\n",
    "    !git clone https://github.com/openknowledge/workshop-genai-camp-data.git\n",
    "    %cd workshop-genai-camp-data\n",
    "    !git lfs pull \n",
    "    ROOT_PATH = \"/content/workshop-genai-camp-data/day-03\"\n",
    "else:\n",
    "    ROOT_PATH = \"..\"\n",
    "DATA_PATH = ROOT_PATH + \"/data\"\n",
    "EVALUATION_PATH = ROOT_PATH + \"/evaluation\"\n",
    "KNOWLEDGEBASE_PATH = ROOT_PATH + \"/knowledgebase\"\n",
    "BOOK_CATALOG_FILE = DATA_PATH + \"/books.json\"\n",
    "TESTDATA_FILE = EVALUATION_PATH + \"/synthetic_testset.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9414c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default models\n",
    "GENERATION_MODEL = \"gemini-1.5-flash\"\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
    "\n",
    "# Set default values for model, model parameters and prompt\n",
    "DEFAULT_CONFIG_TEMPERATURE = 0.9 \n",
    "DEFAULT_CONFIG_TOP_K = 1\n",
    "DEFAULT_CONFIG_MAX_OUTPUT_TOKENS = 200 \n",
    "DEFAULT_SYSTEM_PROMPT = \"Your are a friendly assistant\"\n",
    "DEFAULT_USER_PROMPT = \" \"\n",
    "\n",
    "# Set defaults for rag\n",
    "DEFAULT_K = 3\n",
    "DEFAULT_CHUNK_SIZE = 2000\n",
    "DEFAULT_CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gemini_completion(\n",
    "        model_name: str = GENERATION_MODEL, \n",
    "        temperature:float = DEFAULT_CONFIG_TEMPERATURE,\n",
    "        top_k: int = DEFAULT_CONFIG_TOP_K, \n",
    "        max_output_tokens: int = DEFAULT_CONFIG_MAX_OUTPUT_TOKENS, \n",
    "        system_prompt : str = DEFAULT_SYSTEM_PROMPT, \n",
    "        user_prompt : str = DEFAULT_USER_PROMPT,\n",
    "        verbose: bool = False\n",
    "        ) -> str: \n",
    "    \n",
    "    \"\"\" Calls a gemini model with a given set of parameters and returns the completions \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str, optional [default: DEFAULT_GEMINI_MODEL]\n",
    "        The name of the model to use for the completion\n",
    "    temperature : float, optional [default: DEFAULT_CONFIG_TEMPERATURE]\n",
    "        The temperature of the model\n",
    "    top_k : int, optional [default: DEFAULT_CONFIG_TOP_K]\n",
    "        The number of most recent matches to return\n",
    "    max_output_tokens : int, optional [default: DEFAULT_CONFIG_MAX_OUTPUT_TOKENS]\n",
    "        The maximum number of output tokens to return\n",
    "    system_prompt : str, optional [default: DEFAULT_SYSTEM_PROMPT]\n",
    "        The system prompt to use for the completion\n",
    "    user_prompt : str, optional [default: DEFAULT_USER_PROMPT]\n",
    "        The user prompt to use for the completion\n",
    "    verbose : bool, optional [default: False]\n",
    "        Whether to print details of the completion process or not. Defaults to False            \n",
    "    Returns \n",
    "    -------\n",
    "    str :\n",
    "        the generated text      \n",
    "    \"\"\"    \n",
    "    if verbose: \n",
    "        # print out summary of input values / parameters\n",
    "        print(f'Generating answer for following config:')\n",
    "        print(f'  - SYSTEM PROMPT used:\\n {system_prompt}')\n",
    "        print(f'  - USER PROMPT used:\\n {user_prompt}')\n",
    "        print(f'  - MODEL used:\\n {model_name} (temperature = {temperature}, top_k = {top_k}, max_output_tokens = {max_output_tokens})')\n",
    "\n",
    "    # create generation config \n",
    "    model_config = types.GenerateContentConfig(\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        system_instruction=system_prompt,\n",
    "    )\n",
    "    \n",
    "    # create generation request\n",
    "    response = client.models.generate_content(\n",
    "        model=model_name,\n",
    "        contents=user_prompt,\n",
    "        config=model_config,\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_objects_from_json(file_path: str, cls: BaseModel) -> list:\n",
    "    \"\"\"Reads list of objects from a JSON file and returns the list.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        objects = [cls(**item) for item in data]\n",
    "    return objects\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    \"\"\"Represents the metadata of a document which is stored in the knowledgebase.\"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    pub_year: int\n",
    "\n",
    "class Book(BaseModel):\n",
    "    \"\"\"Represents a book with its metadata.\"\"\"\n",
    "    metadata: Metadata\n",
    "    summary: str\n",
    "\n",
    "class TestdataItem(BaseModel):\n",
    "    \"\"\"Represents a test data item with its input, output, and source.\"\"\"\n",
    "    input: str\n",
    "    output: str\n",
    "    source: Book   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916cfda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG building blocks\n",
    "\n",
    "# This will be the chromadb collection we use as a knowledge base. We do not need the client.\n",
    "chromadb_collection = PersistentClient(path=KNOWLEDGEBASE_PATH).get_or_create_collection(name=\"default\")\n",
    "\n",
    "class FetchedChunk(BaseModel):\n",
    "    \"\"\"Represents a chunk fetched from the knowledgebase.\"\"\"\n",
    "    chunk: str\n",
    "    metadata: Metadata\n",
    "\n",
    "# Building Block \"Embedding\": Create multi dimensional embeddings for a given chunk.\n",
    "def do_embed(chunk: str) -> list[float]:\n",
    "  \"\"\" Embeds a given chunk and returns the embedding\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  chunk : str\n",
    "      The chunk to be embedded\n",
    "  Returns\n",
    "  -------\n",
    "  embedding: [float]\n",
    "      The created embedding\n",
    "  \"\"\"\n",
    "  content_embeddings = client.models.embed_content(model=EMBEDDING_MODEL, contents=chunk).embeddings\n",
    "  return content_embeddings[0].values\n",
    "\n",
    "# Building Block \"Augmentation\": Create an updated prompt by merging the original user input with the provided context\n",
    "# Attention: We manipulated the augmented prompt in order to see the guardrails in action\n",
    "def augment(user_input: str, context: list[str]) -> str:\n",
    "  \"\"\" Augments a given user input by merging it with the provided context and returns the augmented prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be augmented\n",
    "  context : [str]\n",
    "      The context to be merged with the user input\n",
    "  Returns\n",
    "  -------\n",
    "  augmented_prompt: str\n",
    "      The created augmented prompt\n",
    "  \"\"\"\n",
    "  prepared_context = \"\\n\".join(context)\n",
    "  augmented_prompt = f\"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n{prepared_context}?\\n\n",
    "    Question: \\n{user_input}\\n\n",
    "\n",
    "    Answer:\n",
    "  \"\"\"\n",
    "  return augmented_prompt\n",
    "\n",
    "# Building Block \"Top-k Fetching\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "def do_top_k_fetching(user_input_embedding: list[float], top_k: int) -> list[FetchedChunk]:\n",
    "  \"\"\" Fetches the k semantically closest chunks to the user input from the knowledgebase\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input_embedding : [float]\n",
    "      The embedding of the user input\n",
    "  top_k : int\n",
    "      The number of semantically closest chunks to be fetched\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Since we will do the fetching always only for one user_input,\n",
    "  # instead of querying for multiple embeddings simultanously as allowed by the choma API,\n",
    "  # we add the embeddings below to a list and return only the first document (chunk)\n",
    "  \n",
    "  query_result = chromadb_collection.query(\n",
    "      query_embeddings=[user_input_embedding],\n",
    "      n_results=top_k,\n",
    "  )\n",
    "  chunks = query_result[\"documents\"][0]\n",
    "  metadatas = query_result[\"metadatas\"][0]\n",
    "  \n",
    "  fetched_chunks = []\n",
    "  for i in range(len(chunks)):\n",
    "    chunk = chunks[i]\n",
    "    metadata = metadatas[i]\n",
    "    fetched_chunk = FetchedChunk(chunk=chunk, metadata=Metadata(**metadata))\n",
    "    fetched_chunks.append(fetched_chunk)\n",
    "  return fetched_chunks\n",
    "\n",
    "# Building Block \"Generation\": Use the generation model to create a response\n",
    "def generate_response(prompt: str) -> str:\n",
    "  \"\"\" Generates a response for a given prompt\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  prompt : str\n",
    "      The prompt to be used for the generation\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  \"\"\"\n",
    "  return generate_gemini_completion(\n",
    "      model_name=GENERATION_MODEL,\n",
    "      user_prompt=prompt,\n",
    "  )\n",
    "\n",
    "# The rag function should now return the response and the context in order to be evaluated further\n",
    "def do_rag(user_input: str, top_k: int, retrieval_only: bool = False) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(chunk=user_input)\n",
    "\n",
    "  # \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  fetched_chunks = do_top_k_fetching(user_input_embedding=user_input_embedding, top_k=top_k)\n",
    "  context = [chunk.chunk for chunk in fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  if not retrieval_only:\n",
    "    # Generate a response using the augmented prompt\n",
    "    response = generate_response(prompt=augmented_prompt)\n",
    "  else:\n",
    "    response = \"No response generated. Only retrieval was requested.\"\n",
    "\n",
    "  return (response, fetched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7befbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hit_rate(ground_truth, retrieved: list[str]):\n",
    "    gt_set = set(ground_truth)\n",
    "    retrieved_set = set(retrieved)\n",
    "    return int(bool(gt_set & retrieved_set))\n",
    "\n",
    "def calculate_reciprocal_rank(ground_truth: list[str], retrieved: list[str]) -> float:\n",
    "    gt_set = set(ground_truth)\n",
    "    for rank, item in enumerate(retrieved, start=1):\n",
    "        if item in gt_set:\n",
    "            return 1 / rank\n",
    "    return 0.0\n",
    "    \n",
    "def calculate_precision(ground_truth: list[str], retrieved: list[str]) -> float:\n",
    "    k = len(retrieved)\n",
    "    gt_set = set(ground_truth)\n",
    "    retrieved_set = set(retrieved[:k])\n",
    "    return len(gt_set & retrieved_set) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationResult(BaseModel):\n",
    "    \"\"\"Represents the evaluation result with its metrics.\"\"\"\n",
    "    mean_hit_rate: float\n",
    "    mean_reciprocal_rank: float\n",
    "    mean_precision: float\n",
    "\n",
    "def run_evaluation(top_k: int, testdata_items: list[TestdataItem], rag_function: any) -> EvaluationResult:\n",
    "    hit_rates = []\n",
    "    reciprocal_ranks = []\n",
    "    precisions = []\n",
    "\n",
    "    for testdata_item in testdata_items:\n",
    "\n",
    "        # Prepare the ground truth\n",
    "        ground_truth = [testdata_item.source.metadata.title]\n",
    "\n",
    "        # Run the RAG pipeline\n",
    "        _, fetched_chunks = rag_function(user_input=testdata_item.input, top_k=top_k, retrieval_only=True)\n",
    "\n",
    "        # Check if the ground truth is in the context\n",
    "        retrieved = [item.metadata.title for item in fetched_chunks]\n",
    "\n",
    "        # Calculate metrics\n",
    "        # Hit rate\n",
    "        hit_rate_value = calculate_hit_rate(ground_truth=ground_truth, retrieved=retrieved)\n",
    "        hit_rates.append(hit_rate_value)\n",
    "        \n",
    "        # Reciprocal rank\n",
    "        reciprocal_rank = calculate_reciprocal_rank(ground_truth=ground_truth, retrieved=retrieved)\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "\n",
    "        # Precision\n",
    "        precision = calculate_precision(ground_truth=ground_truth, retrieved=retrieved)\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    # Calculate mean values\n",
    "    mean_hit_rate = sum(hit_rates) / len(hit_rates)\n",
    "    mean_reciprocal_rank = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    mean_precision = sum(precisions) / len(precisions)\n",
    "    return EvaluationResult(\n",
    "        mean_hit_rate=mean_hit_rate,\n",
    "        mean_reciprocal_rank=mean_reciprocal_rank,\n",
    "        mean_precision=mean_precision\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537b3ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data for later evaluating optimization techniques\n",
    "testdata_items = read_objects_from_json(file_path=TESTDATA_FILE, cls=TestdataItem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aaee06",
   "metadata": {},
   "source": [
    "### Exercise 01: Hypothetical Document Embeddings (HyDE)\n",
    "HyDE is a technique for improving retrieval. To do this, hypothetical documents are generated based on the user's input that could potentially solve the user's task. These generated documents should closely resemble the documents or chunks in the knowledge base. The hypothetical, generated documents are then used for semantic retrieval.  \n",
    "The underlying idea is that, although the generated documents may not be factually accurate, they are often semantically closer to relevant knowledge base entries than the user's original query. In practice, multiple hypothetical documents are often generated, embedded, and then averaged over each embedding dimension to create a single embedding used for retrieval. This averaging is intended to neutralize incorrect facts while enhancing semantic similarity to the knowledge base.  \n",
    "Your task is to implement this method. Afterwards, reflect on the potential drawbacks of this approach and evaluate the performance of the resulting system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYDE_GENERATION_MODEL = \"gemini-2.0-flash-lite\"\n",
    "\n",
    "# TODO: Implement a function which creates hypothetical summaries of books from a given user query\n",
    "def generate_hypothetical_documents(user_input: str) -> str:\n",
    "    \"\"\" Generates a hypothetical book summary for a given user input\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user_input : str\n",
    "        The user input to be used for the generation\n",
    "    Returns\n",
    "    -------\n",
    "    response: str\n",
    "        The generated hypothetical book summary\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Use your promp engineering skills to create a prompt for\n",
    "    # generating a hypothetical book summary from a given customer query.\n",
    "    # As an example, you can use a real book summary from the book catalog\n",
    "    # CUSTOMER: \"I’m looking for an old book about a scientist who creates a creature out of dead body parts! The creature comes to life and things go very badly.\"\n",
    "    # SUMMARY:\"Frankenstein; Or, The Modern Prometheus\" by Mary Wollstonecraft Shelley is a novel written in the early 19th century. The story explores themes of ambition, the quest for knowledge, and the consequences of man's hubris through the experiences of Victor Frankenstein and the monstrous creation of his own making.The opening of the book introduces Robert Walton, an ambitious explorer on a quest to discover new lands and knowledge in the icy regions of the Arctic. In his letters to his sister Margaret, he expresses both enthusiasm and the fear of isolation in his grand venture. As Walton's expedition progresses, he encounters a mysterious, emaciated stranger who has faced great suffering—furthering the intrigue of his narrative. This stranger ultimately reveals his tale of creation, loss, and the profound consequences of seeking knowledge that lies beyond human bounds. The narrative is set up in a manner that suggests a deep examination of the emotions and ethical dilemmas faced by those who dare to defy the natural order.\n",
    "    hyde_prompt = f\"\"\"\n",
    "        TODO\n",
    "\n",
    "        CUSTOMER:\n",
    "        {user_input}\n",
    "        \n",
    "        SUMMARY:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a hypothetical book summary using the user input as a prompt\n",
    "    hypothetical_summary = generate_gemini_completion(\n",
    "        model_name=HYDE_GENERATION_MODEL,\n",
    "        user_prompt=hyde_prompt,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    \n",
    "    return hypothetical_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the following function to use the new generate_hypothetical_documents function\n",
    "def do_hyde_rag(user_input: str, top_k: int, retrieval_only: bool = False, verbose: bool = False) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "\n",
    "  # TODO: Generate a hypothetical book summary using the user input as a prompt\n",
    "  hypothetical_summary = \n",
    "\n",
    "  # TODO: Embed the hypothetical book summaries\n",
    "  hypothetical_summary_embedding = \n",
    "\n",
    "  # TODO: \"R\" like \"Retrieval\": Get the top-k semantically closest chunks to the hypothetical summary\n",
    "  fetched_chunks = do_top_k_fetching(top_k=top_k)\n",
    "  context = [chunk.chunk for chunk in fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  if not retrieval_only:\n",
    "    # Generate a response using the augmented prompt\n",
    "    response = generate_response(prompt=augmented_prompt)\n",
    "  else:\n",
    "    response = \"No response generated. Only retrieval was requested.\"\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"User input: {user_input}\")\n",
    "    print(f\"Generated hypothetical summary: {hypothetical_summary}\")\n",
    "    print(f\"Fetched chunks: {fetched_chunks}\")\n",
    "    print(f\"Augmented prompt: {augmented_prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "  return (response, fetched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1b330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the do_hyde_rag function with a user input\n",
    "user_prompt = \"I'm looking for a book that takes place in London. I think it's about a crime that happened on a foggy night?\" \n",
    "response, context = do_hyde_rag(user_input=user_prompt, top_k=DEFAULT_K, retrieval_only=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4da135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the new system. How does it perform compared to the original RAG system? What might be the reasons for the differences?\n",
    "evaluation_result = run_evaluation(\n",
    "    top_k=DEFAULT_K,\n",
    "    testdata_items=testdata_items,\n",
    "    rag_function=do_hyde_rag\n",
    ")\n",
    "\n",
    "print(f\"Mean Hit Rate: {evaluation_result.mean_hit_rate:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank: {evaluation_result.mean_reciprocal_rank:.2f}\")\n",
    "print(f\"Mean Precision: {evaluation_result.mean_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e7684",
   "metadata": {},
   "source": [
    "### Exercise 02: Reranking\n",
    "In a RAG system, rerankers play a crucial role in improving the quality of retrieved information before it's passed to the language model. While the retriever fetches potentially relevant documents based on semantic similarity, it often returns noisy or loosely related results. Rerankers evaluate and reorder these documents using more advanced scoring—often based on cross-encoders or task-specific relevance signals—ensuring that the most relevant, contextually appropriate content is prioritized. This leads to more accurate, coherent, and trustworthy generated outputs.  \n",
    "Your task is to use a reranker from Hugging Face to reorder the retrieved chunks. Then, integrate the reranking step into the existing RAG function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b66289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run this the first time, get your free API-Key at https://jina.ai/reranker/\n",
    "if COLAB:\n",
    "    JINA_API_KEY=userdata.get('JINA_API_KEY')\n",
    "\n",
    "# Here we use the Jina API to rerank the documents\n",
    "def use_reranker(query:str, documents:list[str], top_n: int) -> list[int]:\n",
    "  url = \"https://api.jina.ai/v1/rerank\"\n",
    "  headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {JINA_API_KEY}\"\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "      \"model\": \"jina-reranker-v2-base-multilingual\",\n",
    "      \"query\": query,\n",
    "      \"top_n\": top_n,\n",
    "      \"documents\": documents,\n",
    "      \"return_documents\": False\n",
    "  }\n",
    "\n",
    "  # Make the request\n",
    "  response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "  results = json.loads(response.text)[\"results\"]\n",
    "  return [result[\"index\"] for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a293a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the reranking function, which uses the CrossEncoder model to rerank the fetched chunks\n",
    "def rerank(user_input: str, fetched_chunks: list[FetchedChunk]) -> list[FetchedChunk]:\n",
    "    \"\"\" Reranks the fetched chunks based on the user input and returns the reranked chunks\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    user_input : str\n",
    "        The user input to be used for the reranking\n",
    "    fetched_chunks : [FetchedChunk]\n",
    "        The fetched chunks to be reranked\n",
    "    Returns\n",
    "    -------\n",
    "    reranked_chunks: [FetchedChunk]\n",
    "        The reranked chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Rank the fetched chunks based on the user input\n",
    "    documents = [chunk.chunk for chunk in fetched_chunks]\n",
    "    rankings = use_reranker(\n",
    "        query=user_input,\n",
    "        documents=documents,\n",
    "        top_n=len(documents)\n",
    "    )\n",
    "\n",
    "    # TODO: Sort the fetched chunks based on the rankings and return the reranked chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbb4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update the do_reranked_rag function to use the reranking function\n",
    "def do_reranked_rag(user_input: str, top_k: int, retrieval_only: bool = False, verbose: bool = False) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(chunk=user_input)\n",
    "\n",
    "  # \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  fetched_chunks = do_top_k_fetching(user_input_embedding=user_input_embedding, top_k=top_k)\n",
    "\n",
    "  # TODO: Rerank the fetched chunks based on the user input\n",
    "  ranked_fetched_chunks = \n",
    "  context = [chunk.chunk for chunk in ranked_fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  if not retrieval_only:\n",
    "    # Generate a response using the augmented prompt\n",
    "    response = generate_response(prompt=augmented_prompt)\n",
    "  else:\n",
    "    response = \"No response generated. Only retrieval was requested.\"\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"User input: {user_input}\")\n",
    "    print(f\"Fetched chunks: {fetched_chunks}\")\n",
    "    print(f\"Ranked fetched chunks: {ranked_fetched_chunks}\")\n",
    "    print(f\"Augmented prompt: {augmented_prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "  return (response, ranked_fetched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635cf617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the new system. How does it perform compared to the original RAG system?\n",
    "evaluation_result = run_evaluation(\n",
    "    top_k=DEFAULT_K,\n",
    "    testdata_items=testdata_items,\n",
    "    rag_function=do_reranked_rag\n",
    ")\n",
    "print(f\"Mean Hit Rate: {evaluation_result.mean_hit_rate:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank: {evaluation_result.mean_reciprocal_rank:.2f}\")\n",
    "print(f\"Mean Precision: {evaluation_result.mean_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac72aef",
   "metadata": {},
   "source": [
    "### Exercise 03: Improving Reranked RAG\n",
    "Using the reranker seems to improve quality. Unfortunately, precision is still very low. Can you think about a way to keep reciprocal rank and hit rate high, while also increasing precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0aec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve the rag system even further. How can we improve precision while keeping hit rate and reciprocal rank high?\n",
    "def do_reranked_rag_v2(user_input: str, top_k: int, retrieval_only: bool = False, verbose: bool = False) -> tuple[str, list[str]]:\n",
    "  \"\"\" Runs the RAG pipeline with a given user input and returns the response and the context\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  user_input : str\n",
    "      The user input to be used for the RAG pipeline\n",
    "  Returns\n",
    "  -------\n",
    "  response: str\n",
    "      The generated response\n",
    "  context: [str]\n",
    "      The fetched chunks\n",
    "  \"\"\"\n",
    "\n",
    "  # Embed the user input\n",
    "  user_input_embedding = do_embed(chunk=user_input)\n",
    "\n",
    "  # \"R\" like \"Retrieval\": Get the k semantically closest chunks to the user input from the knowledgebase\n",
    "  # TODO: Fetch more chunks than before\n",
    "  fetched_chunks = \n",
    "\n",
    "  # TODO: Rerank the fetched chunks and only keep the top_k chunks\n",
    "  ranked_fetched_chunks = \n",
    "  context = [chunk.chunk for chunk in ranked_fetched_chunks]\n",
    "\n",
    "  # \"A\" like \"Augmented\": Create the augmented prompt\n",
    "  augmented_prompt = augment(user_input=user_input, context=context)\n",
    "\n",
    "  # \"G\" like \"Generation\": Generate a response\n",
    "  if not retrieval_only:\n",
    "    # Generate a response using the augmented prompt\n",
    "    response = generate_response(prompt=augmented_prompt)\n",
    "  else:\n",
    "    response = \"No response generated. Only retrieval was requested.\"\n",
    "\n",
    "  if verbose:\n",
    "    print(f\"User input: {user_input}\")\n",
    "    print(f\"Fetched chunks: {fetched_chunks}\")\n",
    "    print(f\"Ranked fetched chunks: {ranked_fetched_chunks}\")\n",
    "    print(f\"Augmented prompt: {augmented_prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "  return (response, ranked_fetched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e80464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the new system. How does it perform compared to the original RAG system?\n",
    "evaluation_result = run_evaluation(\n",
    "    top_k=DEFAULT_K,\n",
    "    testdata_items=testdata_items,\n",
    "    rag_function=do_reranked_rag_v2\n",
    ")\n",
    "print(f\"Mean Hit Rate: {evaluation_result.mean_hit_rate:.2f}\")\n",
    "print(f\"Mean Reciprocal Rank: {evaluation_result.mean_reciprocal_rank:.2f}\")\n",
    "print(f\"Mean Precision: {evaluation_result.mean_precision:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
